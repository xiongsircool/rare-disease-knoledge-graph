#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆPMCå…¨æ–‡ä¸‹è½½å™¨
æ•´åˆç°æœ‰çš„æ‰¹é‡ä¸‹è½½è„šæœ¬å’Œè§£æåŠŸèƒ½
åŸºäº test/pmc/downloadpmc.py å’Œ test/pmc/pmcpaser.py
"""

import os
import re
import math
import time
import json
from pathlib import Path
from typing import List, Dict, Optional, Set
from urllib.error import HTTPError, URLError
from http.client import IncompleteRead
from dataclasses import dataclass, asdict
from Bio import Entrez
import xml.etree.ElementTree as ET


@dataclass
class OptimizedPMCConfig:
    """ä¼˜åŒ–ç‰ˆPMCä¸‹è½½é…ç½®"""
    email: str
    api_key: Optional[str] = None
    output_dir: str = "optimized_pmc_data"

    # ä¸‹è½½å‚æ•°ï¼ˆåŸºäºåŸæœ‰è„šæœ¬ä¼˜åŒ–ï¼‰
    batch_size: int = 500  # æ¯æ‰¹ä¸‹è½½çš„æ–‡ç« æ•°
    disease_batch_size: int = 10  # æ¯æ‰¹å¤„ç†çš„ç–¾ç—…æ•°
    max_records_per_search: int = 100000  # PubMedå•æ¬¡æœ€å¤§è®°å½•æ•°

    # æ§åˆ¶å‚æ•°
    sleep_time: float = 0.34  # æ— API keyæ—¶çš„å»¶è¿Ÿ
    sleep_time_with_key: float = 0.12  # æœ‰API keyæ—¶çš„å»¶è¿Ÿ
    max_retry: int = 3

    # è§£æé€‰é¡¹
    save_parsed_json: bool = True
    save_raw_xml: bool = True
    parse_detailed_content: bool = True


class OptimizedPMCArticle:
    """ä¼˜åŒ–ç‰ˆPMCæ–‡ç« æ•°æ®ç±»ï¼ˆåŸºäºåŸæœ‰è§£æå™¨ï¼‰"""

    def __init__(self):
        self.pmc_id = ""
        self.pmid = ""
        self.doi = ""
        self.title = ""
        self.abstract = ""
        self.authors = []
        self.journal = ""
        self.publication_date = {}
        self.article_type = ""
        self.disease = ""
        self.keywords = []
        self.publication_types = []
        self.italic_texts = []
        self.notes = ""
        self.notes_links = []
        self.full_text = ""
        self.figure_info_list = []
        self.table_list = []
        self.reference_list = []

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        try:
            return asdict(self)
        except Exception:
            # å¦‚æœasdictå¤±è´¥ï¼Œæ‰‹åŠ¨è½¬æ¢
            return {
                'pmc_id': self.pmc_id,
                'pmid': self.pmid,
                'doi': self.doi,
                'title': self.title,
                'abstract': self.abstract,
                'authors': self.authors,
                'journal': self.journal,
                'publication_date': self.publication_date,
                'article_type': self.article_type,
                'disease': self.disease,
                'keywords': self.keywords,
                'publication_types': self.publication_types,
                'italic_texts': self.italic_texts,
                'notes': self.notes,
                'notes_links': self.notes_links,
                'full_text': self.full_text,
                'figure_info_list': self.figure_info_list,
                'table_list': self.table_list,
                'reference_list': self.reference_list
            }


class OptimizedPMCDownloader:
    """ä¼˜åŒ–ç‰ˆPMCå…¨æ–‡ä¸‹è½½å™¨"""

    def __init__(self, config: OptimizedPMCConfig):
        self.config = config
        self.setup_entrez()
        self.setup_directories()
        self.processed_pmids: Set[str] = set()

    def setup_entrez(self):
        """è®¾ç½®Entrezé…ç½®"""
        Entrez.email = self.config.email
        Entrez.tool = "optimized_pmc_downloader"
        if self.config.api_key:
            Entrez.api_key = self.config.api_key

    def setup_directories(self):
        """åˆ›å»ºç›®å½•ç»“æ„"""
        self.base_dir = Path(self.config.output_dir)
        self.xml_dir = self.base_dir / "xml_files"
        self.parsed_dir = self.base_dir / "parsed_json"
        self.metadata_dir = self.base_dir / "metadata"

        for dir_path in [self.base_dir, self.xml_dir, self.parsed_dir, self.metadata_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

    def get_sleep_time(self) -> float:
        """è·å–é€‚å½“çš„å»¶è¿Ÿæ—¶é—´"""
        return (self.config.sleep_time_with_key
                if self.config.api_key
                else self.config.sleep_time)

    def _safe_name(self, s: str, maxlen: int = 80) -> str:
        """å®‰å…¨æ–‡ä»¶åï¼ˆæ¥è‡ªåŸè„šæœ¬ï¼‰"""
        s = re.sub(r"[^\w\-\.\+]+", "_", s.strip())
        return s[:maxlen].strip("_") or "query"

    def _retry_call(self, fn, *args, **kwargs):
        """é‡è¯•æœºåˆ¶ï¼ˆæ¥è‡ªåŸè„šæœ¬ï¼‰"""
        attempt = 0
        while True:
            try:
                return fn(*args, **kwargs)
            except (HTTPError, URLError, IncompleteRead) as e:
                attempt += 1
                if attempt > self.config.max_retry:
                    print(f"[ERROR] é‡è¯• {self.config.max_retry} æ¬¡åä»ç„¶å¤±è´¥: {e}")
                    raise
                print(f"[WARN] ç¬¬ {attempt} æ¬¡é‡è¯•ï¼Œé”™è¯¯: {e}")
                time.sleep(self.get_sleep_time() * attempt)

    def _safe_fetch_with_retry(self, batch_ids: List[str], max_retries: int = 3):
        """å®‰å…¨çš„æ‰¹é‡ä¸‹è½½ï¼ˆæ¥è‡ªåŸè„šæœ¬ï¼‰"""
        for attempt in range(max_retries):
            try:
                h = Entrez.efetch(
                    db="pmc",
                    id=",".join(batch_ids),
                    rettype="xml",
                    retmode="text"
                )

                # åˆ†æ®µè¯»å–ï¼Œé¿å… IncompleteRead é”™è¯¯
                xml_parts = []
                chunk_size = 8192  # 8KB chunks

                while True:
                    chunk = h.read(chunk_size)
                    if not chunk:
                        break
                    xml_parts.append(chunk)

                h.close()
                xml_text = b''.join(xml_parts)

                # ç¡®ä¿ xml_text æ˜¯å­—ç¬¦ä¸²ç±»å‹
                if isinstance(xml_text, bytes):
                    xml_text = xml_text.decode('utf-8')

                return xml_text

            except IncompleteRead as e:
                print(f"[WARN] ç¬¬ {attempt + 1} æ¬¡å°è¯•å‡ºç° IncompleteRead é”™è¯¯: {e}")
                if attempt < max_retries - 1:
                    print(f"[INFO] ç­‰å¾… {self.get_sleep_time() * (attempt + 1)} ç§’åé‡è¯•...")
                    time.sleep(self.get_sleep_time() * (attempt + 1))
                    continue
                else:
                    print(f"[ERROR] æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œè·³è¿‡è¿™æ‰¹æ•°æ®")
                    raise
            except Exception as e:
                print(f"[ERROR] ä¸‹è½½æ—¶å‡ºç°æœªçŸ¥é”™è¯¯: {e}")
                raise

    def safe_search_term(self, disease_name: str) -> str:
        """æ„é€ å®‰å…¨çš„PMCæ£€ç´¢å¼ - åŸºäºè¯Šæ–­ç»“æœä¼˜åŒ–"""
        disease_clean = re.sub(r'[^\w\s\-\.]', ' ', disease_name).strip()

        # åŸºäºè¯Šæ–­ç»“æœçš„ç®€å•ç­–ç•¥ï¼šç›´æ¥ä½¿ç”¨ç–¾ç—…åç§°ï¼Œä¸æ·»åŠ å¤æ‚é™åˆ¶
        # è¯Šæ–­ç»“æœæ˜¾ç¤ºå®Œæ•´ç–¾ç—…åç§°åœ¨PMCä¸­æ£€ç´¢æ•ˆæœå¾ˆå¥½

        # ç­–ç•¥1: ç›´æ¥ä½¿ç”¨å®Œæ•´ç–¾ç—…åç§°ï¼ˆä¸é™åˆ¶å­—æ®µï¼Œæ•ˆæœæœ€å¥½ï¼‰
        return f'"{disease_clean}"'

    def _try_search(self, search_term: str) -> tuple[int, dict]:
        """å°è¯•æœç´¢å¹¶è¿”å›ç»“æœæ•°é‡å’Œæœç´¢ç»“æœ"""
        try:
            # å…ˆè·å–æ€»æ•°
            handle = self._retry_call(
                Entrez.esearch,
                db="pmc",
                term=search_term,
                retmax=0,
                usehistory="y"
            )
            search_result = Entrez.read(handle)
            handle.close()

            count = int(search_result["Count"])
            return count, search_result

        except Exception as e:
            print(f"[ERROR] æœç´¢å¤±è´¥: {e}")
            return 0, {}

    def search_pmc_by_disease(self, disease: str) -> List[str]:
        """é€šè¿‡ç–¾ç—…åç§°æœç´¢PMCè·å–æ–‡ç« IDåˆ—è¡¨ï¼ˆåŸºäºåŸè„šæœ¬ä¼˜åŒ–ï¼‰"""
        # ç­–ç•¥1: ç²¾ç¡®æ£€ç´¢ï¼ˆå¸¦å¼•å·ï¼‰
        exact_search_term = self.safe_search_term(disease)
        print(f"[DEBUG] {disease}: PMCç²¾ç¡®æ£€ç´¢å¼ = {exact_search_term}")

        # å°è¯•ç²¾ç¡®æ£€ç´¢
        count, search_result = self._try_search(exact_search_term)
        final_search_term = exact_search_term

        if count == 0:
            # ç­–ç•¥2: å®½æ¾æ£€ç´¢ï¼ˆä¸å¸¦å¼•å·ï¼‰
            disease_clean = re.sub(r'[^\w\s\-\.]', ' ', disease).strip()
            loose_search_term = disease_clean
            print(f"[DEBUG] {disease}: å°è¯•å®½æ¾æ£€ç´¢å¼ = {loose_search_term}")

            count, search_result = self._try_search(loose_search_term)

            if count == 0:
                print(f"[INFO] {disease}: PMCä¸­æœªæ‰¾åˆ°å…è´¹å…¨æ–‡")
                return []
            else:
                print(f"[INFO] {disease}: å®½æ¾æ£€ç´¢æ‰¾åˆ° {count} ç¯‡å…è´¹å…¨æ–‡")
                final_search_term = loose_search_term
        else:
            print(f"[INFO] {disease}: ç²¾ç¡®æ£€ç´¢æ‰¾åˆ° {count} ç¯‡å…è´¹å…¨æ–‡")

        # è·å–æ‰€æœ‰PMC IDï¼ˆåŸºäºåŸè„šæœ¬é€»è¾‘ï¼‰
        all_ids = []
        retstart = 0
        page_size = min(self.config.max_records_per_search, count)

        while retstart < count:
            size = min(page_size, count - retstart)
            handle = self._retry_call(
                Entrez.esearch,
                db="pmc",
                term=final_search_term,
                retstart=retstart,
                retmax=size,
                usehistory="y"
            )
            r = Entrez.read(handle)
            handle.close()

            all_ids.extend([f"PMC{_id}" if not str(_id).upper().startswith("PMC") else str(_id).upper()
                       for _id in r["IdList"]])
            retstart += size
            print(f"[INFO] {disease}: è·å–IDï¼š{len(all_ids)}/{count}")
            time.sleep(self.get_sleep_time())

        return all_ids

    def download_pmc_by_disease(self, disease: str, pmc_ids: List[str]) -> int:
        """ä¸‹è½½å•ä¸ªç–¾ç—…çš„PMCå…¨æ–‡ï¼ˆåŸºäºåŸè„šæœ¬æ ¸å¿ƒé€»è¾‘ï¼‰"""
        if not pmc_ids:
            return 0

        print(f"[INFO] {disease}: å¼€å§‹ä¸‹è½½ï¼Œæ¯æ‰¹ {self.config.batch_size} ç¯‡ï¼Œå…± {math.ceil(len(pmc_ids)/self.config.batch_size)} æ‰¹ã€‚")

        # åˆ›å»ºç–¾ç—…ç›®å½•
        base = self._safe_name(disease)
        disease_dir = self.xml_dir / base
        disease_dir.mkdir(exist_ok=True)

        downloaded_count = 0
        batch_idx = 1
        failed_batches = []

        for i in range(0, len(pmc_ids), self.config.batch_size):
            batch_ids = pmc_ids[i:i+self.config.batch_size]
            print(f"[INFO] æ­£åœ¨ä¸‹è½½ç¬¬ {batch_idx} æ‰¹ï¼ŒåŒ…å« {len(batch_ids)} ç¯‡æ–‡çŒ®...")

            try:
                # ä½¿ç”¨åŸè„šæœ¬çš„ä¸‹è½½æ–¹æ³•
                xml_text = self._safe_fetch_with_retry(batch_ids)

                # ä¿å­˜XMLæ–‡ä»¶ï¼ˆåŸè„šæœ¬é€»è¾‘ï¼‰
                outfile = disease_dir / f"{base}_batch_{batch_idx:05d}.xml"
                with open(outfile, "w", encoding="utf-8") as f:
                    f.write(xml_text)
                print(f"[OK] ä¿å­˜ï¼š{outfile} ï¼ˆæœ¬æ‰¹ {len(batch_ids)} ç¯‡ï¼‰")

                downloaded_count += len(batch_ids)

                # å¦‚æœå¯äº†è§£æï¼Œåˆ™è§£æè¿™ä¸€æ‰¹æ•°æ®
                if self.config.parse_detailed_content:
                    self.parse_and_save_batch(xml_text, disease, batch_idx)

            except Exception as e:
                print(f"[ERROR] ç¬¬ {batch_idx} æ‰¹ä¸‹è½½å¤±è´¥: {e}")
                failed_batches.append((batch_idx, batch_ids, str(e)))

            batch_idx += 1
            time.sleep(self.get_sleep_time())

        print(f"[INFO] {disease}: ä¸‹è½½å®Œæˆï¼æˆåŠŸ {downloaded_count} ç¯‡ï¼Œå¤±è´¥æ‰¹æ¬¡ {len(failed_batches)}")
        return downloaded_count

    def parse_and_save_batch(self, xml_text: str, disease: str, batch_idx: int):
        """è§£ææ‰¹æ¬¡æ•°æ®å¹¶ä¿å­˜ï¼ˆåŸºäºåŸè§£æå™¨ï¼‰"""
        try:
            # ä½¿ç”¨åŸæœ‰çš„è§£æé€»è¾‘
            articles = self.parse_full_articles(xml_text, disease)

            if articles:
                safe_disease_name = self._safe_name(disease)
                timestamp = time.strftime('%Y%m%d_%H%M%S')

                # ä¿å­˜è§£æç»“æœ
                json_file = self.parsed_dir / f"{safe_disease_name}_batch_{batch_idx:05d}_{timestamp}.json"

                data = {
                    'disease': disease,
                    'batch_number': batch_idx,
                    'search_timestamp': timestamp,
                    'total_articles': len(articles),
                    'articles': [article.to_dict() for article in articles]
                }

                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)

                print(f"[OK] è§£æå¹¶ä¿å­˜: {json_file} ({len(articles)} ç¯‡)")

        except Exception as e:
            print(f"[ERROR] è§£ææ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")

    def parse_full_articles(self, xml_content: str, disease: str) -> List[OptimizedPMCArticle]:
        """è§£æå®Œæ•´æ–‡ç« ï¼ˆåŸºäºåŸè§£æå™¨ï¼‰"""
        try:
            if isinstance(xml_content, bytes):
                xml_content = xml_content.decode('utf-8')

            root = ET.fromstring(xml_content)
            articles = []

            # å¤„ç†å‘½åç©ºé—´
            namespace = root.tag.split('}')[0] + '}' if '}' in root.tag else ''
            article_elements = root.findall('article')
            if not article_elements and namespace:
                article_elements = root.findall(f'{namespace}article')

            for article_elem in article_elements:
                try:
                    article = OptimizedPMCArticle()
                    article.disease = disease

                    # ä½¿ç”¨åŸæœ‰è§£æå™¨çš„é€»è¾‘
                    article.pmc_id = self.get_clean_text(article_elem, './/article-id[@pub-id-type="pmcid"]')
                    article.pmid = self.get_clean_text(article_elem, './/article-id[@pub-id-type="pmid"]')
                    article.doi = self.get_clean_text(article_elem, './/article-id[@pub-id-type="doi"]')
                    article.title = self.get_clean_text(article_elem, './/article-title')
                    article.abstract = self.get_clean_text(article_elem, './/abstract')
                    article.authors = self.parse_authors(article_elem)
                    article.journal = self.get_clean_text(article_elem, './/journal-title')
                    article.article_type = article_elem.get('article-type', '')
                    article.publication_date = self.parse_publication_date(article_elem)
                    article.keywords = self.parse_keywords(article_elem)
                    article.publication_types = self.parse_publication_types(article_elem)
                    article.italic_texts = self.parse_italic_texts(article_elem)
                    article.notes = self.get_clean_text(article_elem, './/notes')
                    article.notes_links = self.parse_notes_links(article_elem)

                    # è§£æå…¨æ–‡å†…å®¹
                    article.full_text = self.parse_full_text(article_elem)

                    # è§£æå›¾è¡¨ä¿¡æ¯
                    article.figure_info_list = self.parse_figures(article_elem, article.pmc_id)
                    article.table_list = self.parse_tables(article_elem)

                    # è§£æå‚è€ƒæ–‡çŒ®
                    article.reference_list = self.parse_references(article_elem)

                    if article.pmc_id and article.pmc_id not in self.processed_pmids:
                        articles.append(article)
                        self.processed_pmids.add(article.pmc_id)

                except Exception as e:
                    print(f"[WARN] è§£ææ–‡ç« å¤±è´¥: {e}")
                    continue

            return articles

        except ET.ParseError as e:
            print(f"[ERROR] XMLè§£æå¤±è´¥: {e}")
            return []

    # ä»¥ä¸‹æ–¹æ³•æ¥è‡ªåŸè§£æå™¨
    def get_clean_text(self, element, xpath: str = '.') -> str:
        """å®‰å…¨è·å–æ–‡æœ¬å†…å®¹ï¼ˆæ¥è‡ªåŸè§£æå™¨ï¼‰"""
        if element is None:
            return ""
        elem = element.find(xpath)
        return elem.text.strip() if elem is not None and elem.text else ""

    def parse_authors(self, article_elem) -> List[str]:
        """è§£æä½œè€…ä¿¡æ¯ï¼ˆæ¥è‡ªåŸè§£æå™¨ï¼‰"""
        authors = []
        for author in article_elem.findall('.//Author'):
            last_name = self.get_clean_text(author, './/LastName')
            fore_name = self.get_clean_text(author, './/ForeName')
            if last_name:
                author_name = f"{last_name} {fore_name}".strip()
                authors.append(author_name)
        return authors

    def parse_publication_date(self, article_elem) -> Dict:
        """è§£æå‘è¡¨æ—¥æœŸï¼ˆæ¥è‡ªåŸè§£æå™¨ï¼‰"""
        pub_date = article_elem.find('.//pub-date')
        if pub_date is not None:
            year = self.get_clean_text(pub_date, './/year')
            month = self.get_clean_text(pub_date, './/month')
            day = self.get_clean_text(pub_date, './/day')

            date_parts = []
            if year:
                date_parts.append(year)
            if month:
                date_parts.append(month.zfill(2))
            if day:
                date_parts.append(day.zfill(2))

            return {
                'year': year,
                'month': month,
                'day': day,
                'formatted': '-'.join(date_parts) if date_parts else year
            }
        return {}

    def parse_keywords(self, article_elem) -> List[str]:
        """è§£æå…³é”®è¯ï¼ˆæ¥è‡ªåŸè§£æå™¨ï¼‰"""
        keywords = []
        for keyword in article_elem.findall('.//kwd'):
            if keyword.text:
                keywords.append(keyword.text.strip())
        return keywords

    def parse_publication_types(self, article_elem) -> List[str]:
        """è§£æå‘è¡¨ç±»å‹ï¼ˆæ¥è‡ªåŸè§£æå™¨ï¼‰"""
        pub_types = []
        for pub_type in article_elem.findall('.//PublicationType'):
            if pub_type.text:
                pub_types.append(pub_type.text.strip())
        return pub_types

    def parse_italic_texts(self, article_elem) -> List[str]:
        """è§£ææ–œä½“æ–‡æœ¬ï¼ˆæ¥è‡ªåŸè§£æå™¨ï¼‰"""
        italic_texts = []
        title_elem = article_elem.find('.//article-title')
        if title_elem is not None:
            for italic in title_elem.findall('.//italic'):
                text = self.get_clean_text(ET.Element('dummy', text=italic.text)) if italic.text else ""
                if text and len(text) > 1:
                    italic_texts.append(text)

        abstract_elem = article_elem.find('.//abstract')
        if abstract_elem is not None:
            for italic in abstract_elem.findall('.//italic'):
                text = self.get_clean_text(ET.Element('dummy', text=italic.text)) if italic.text else ""
                if text and len(text) > 1:
                    italic_texts.append(text)

        return list(set(italic_texts))

    def parse_notes_links(self, article_elem) -> List[str]:
        """è§£æç¬”è®°é“¾æ¥ï¼ˆæ¥è‡ªåŸè§£æå™¨ï¼‰"""
        notes_links = []
        notes_elem = article_elem.find('.//notes')
        if notes_elem is not None:
            for ext_link in notes_elem.findall('.//ext-link'):
                href = ext_link.get('{http://www.w3.org/1999/xlink}href')
                if href:
                    notes_links.append(href)
        return notes_links

    def parse_full_text(self, article_elem) -> str:
        """è§£æå…¨æ–‡å†…å®¹"""
        text_parts = []
        body_elem = article_elem.find('.//body')
        if body_elem is not None:
            for p in body_elem.findall('.//p'):
                text = ''.join(p.itertext()).strip()
                if text:
                    text_parts.append(text)
        return ' '.join(text_parts)

    def parse_figures(self, article_elem, pmc_id: str) -> List[Dict]:
        """è§£æå›¾è¡¨ä¿¡æ¯ï¼ˆæ¥è‡ªåŸè§£æå™¨ï¼‰"""
        figures = []
        for fig in article_elem.findall('.//fig'):
            fig_info = {
                'id': fig.get('id', ''),
                'label': self.get_clean_text(fig, './/label'),
                'title': self.get_clean_text(fig, './/title'),
                'caption': self.get_clean_text(fig, './/caption'),
                'graphic_url': '',
                'download_url': ''
            }

            graphic = fig.find('.//graphic')
            if graphic is not None:
                graphic_url = graphic.get('{http://www.w3.org/1999/xlink}href', '') or graphic.get('href', '')
                fig_info['graphic_url'] = graphic_url

                if pmc_id and graphic_url:
                    fig_info['download_url'] = f"https://www.ncbi.nlm.nih.gov/pmc/articles/instance/{pmc_id.replace('PMC', '')}/bin/{graphic_url}"

            if fig_info['id']:
                figures.append(fig_info)
        return figures

    def parse_tables(self, article_elem) -> List[Dict]:
        """è§£æè¡¨æ ¼ä¿¡æ¯ï¼ˆæ¥è‡ªåŸè§£æå™¨ï¼‰"""
        tables = []
        for table_wrap in article_elem.findall('.//table-wrap'):
            table_info = {
                'id': table_wrap.get('id', ''),
                'label': self.get_clean_text(table_wrap, './/label'),
                'caption': self.get_clean_text(table_wrap, './/caption'),
                'rows': []
            }

            table = table_wrap.find('.//table')
            if table is not None:
                for row in table.findall('.//tr'):
                    row_data = []
                    for cell in row.findall('.//td|.//th'):
                        cell_text = ''.join(cell.itertext()).strip()
                        if cell_text:
                            row_data.append(cell_text)
                    if row_data:
                        table_info['rows'].append(row_data)

            if table_info['id']:
                tables.append(table_info)
        return tables

    def parse_references(self, article_elem) -> List[Dict]:
        """è§£æå‚è€ƒæ–‡çŒ®ï¼ˆåŸºäºåŸè§£æå™¨é€»è¾‘ï¼‰"""
        references = []
        ref_list = article_elem.find('.//ref-list')
        if ref_list is not None:
            for ref in ref_list.findall('.//ref'):
                ref_info = {
                    'label': self.get_clean_text(ref, './/label'),
                    'authors': '',
                    'article_title': self.get_clean_text(ref, './/article-title'),
                    'source': self.get_clean_text(ref, './/source'),
                    'year': self.get_clean_text(ref, './/year'),
                    'volume': self.get_clean_text(ref, './/volume'),
                    'issue': self.get_clean_text(ref, './/issue'),
                    'fpage': self.get_clean_text(ref, './/fpage'),
                    'lpage': self.get_clean_text(ref, './/lpage'),
                    'doi': self.get_clean_text(ref, './/pub-id[@pub-id-type="doi"]'),
                    'pmid': self.get_clean_text(ref, './/pub-id[@pub-id-type="pmid"]')
                }
                references.append(ref_info)
        return references

    def save_parsed_articles(self, articles: List[OptimizedPMCArticle], identifier: str):
        """ä¿å­˜è§£æåçš„æ–‡ç« æ•°æ®"""
        if not articles or not self.config.save_parsed_json:
            return

        timestamp = time.strftime('%Y%m%d_%H%M%S')
        json_file = self.parsed_dir / f"{identifier}_{timestamp}.json"

        data = {
            'identifier': identifier,
            'search_timestamp': timestamp,
            'total_articles': len(articles),
            'articles': [article.to_dict() for article in articles]
        }

        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"[OK] ä¿å­˜è§£ææ•°æ®: {json_file} ({len(articles)} ç¯‡)")

    def process_single_disease(self, disease: str) -> Dict:
        """å¤„ç†å•ä¸ªç–¾ç—…"""
        print(f"\n{'='*60}")
        print(f"ğŸ”¬ å¤„ç†ç–¾ç—…: {disease}")
        print(f"{'='*60}")

        result = {
            'disease': disease,
            'success': False,
            'pmc_ids_found': 0,
            'articles_downloaded': 0,
            'error': None,
            'processing_time': 0
        }

        start_time = time.time()

        try:
            # æœç´¢PMC
            pmc_ids = self.search_pmc_by_disease(disease)
            result['pmc_ids_found'] = len(pmc_ids)

            if not pmc_ids:
                result['success'] = True
                print(f"[INFO] {disease}: PMCä¸­æœªæ‰¾åˆ°å…è´¹å…¨æ–‡")
                return result

            # ä¸‹è½½å…¨æ–‡
            downloaded_count = self.download_pmc_by_disease(disease, pmc_ids)
            result['articles_downloaded'] = downloaded_count

            result['success'] = True
            print(f"[OK] {disease}: å®Œæˆï¼Œä¸‹è½½ {downloaded_count} ç¯‡")

        except Exception as e:
            result['error'] = str(e)
            print(f"[ERROR] {disease}: å¤„ç†å¤±è´¥ - {e}")

        finally:
            result['processing_time'] = time.time() - start_time

        return result

    def process_diseases_batch(self, diseases: List[str]) -> List[Dict]:
        """æ‰¹é‡å¤„ç†ç–¾ç—…"""
        print(f"\nğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆPMCæ‰¹é‡å¤„ç† {len(diseases)} ä¸ªç–¾ç—…")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.base_dir}")

        results = []

        for i, disease in enumerate(diseases, 1):
            print(f"\nğŸ“‹ è¿›åº¦: {i}/{len(diseases)} - {disease}")
            result = self.process_single_disease(disease)
            results.append(result)

        # ä¿å­˜æ‰¹å¤„ç†ç»“æœ
        self.save_batch_results(results)
        self.print_batch_summary(results)

        return results

    def save_batch_results(self, results: List[Dict]):
        """ä¿å­˜æ‰¹å¤„ç†ç»“æœ"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        results_file = self.metadata_dir / f"optimized_pmc_results_{timestamp}.json"

        summary = {
            'timestamp': timestamp,
            'total_diseases': len(results),
            'successful_diseases': sum(1 for r in results if r['success']),
            'total_pmc_ids': sum(r['pmc_ids_found'] for r in results),
            'total_articles': sum(r['articles_downloaded'] for r in results),
            'total_processing_time': sum(r['processing_time'] for r in results),
            'failed_diseases': [r['disease'] for r in results if not r['success']],
            'detailed_results': results
        }

        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        print(f"\n[INFO] ä¼˜åŒ–ç‰ˆPMCæ‰¹å¤„ç†ç»“æœå·²ä¿å­˜: {results_file}")

    def print_batch_summary(self, results: List[Dict]):
        """æ‰“å°æ‰¹å¤„ç†æ€»ç»“"""
        print("\n" + "="*80)
        print("ğŸ“Š ä¼˜åŒ–ç‰ˆPMCæ‰¹é‡ä¸‹è½½å®Œæˆæ€»ç»“")
        print("="*80)

        successful = [r for r in results if r['success']]
        failed = [r for r in results if not r['success']]

        print(f"ğŸ“… å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ”¬ æ€»ç–¾ç—…æ•°: {len(results)}")
        print(f"âœ… æˆåŠŸå¤„ç†: {len(successful)}")
        print(f"âŒ å¤„ç†å¤±è´¥: {len(failed)}")

        if successful:
            total_pmc_ids = sum(r['pmc_ids_found'] for r in successful)
            total_articles = sum(r['articles_downloaded'] for r in successful)
            total_time = sum(r['processing_time'] for r in successful)

            print(f"ğŸ“Š æ‰¾åˆ°PMC ID: {total_pmc_ids}")
            print(f"ğŸ“„ ä¸‹è½½å…¨æ–‡: {total_articles}")
            print(f"â±ï¸  æ€»ç”¨æ—¶: {total_time:.1f} ç§’")

        if failed:
            print(f"\nâŒ å¤±è´¥çš„ç–¾ç—…:")
            for result in failed[:5]:
                print(f"   - {result['disease']}: {result.get('error', 'Unknown error')}")

        print(f"\nğŸ“ æ•°æ®ä¿å­˜åœ¨: {self.base_dir}")
        print("="*80)


def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    print("ğŸ§¬ ä¼˜åŒ–ç‰ˆPMCå…¨æ–‡ä¸‹è½½å™¨ç¤ºä¾‹")
    print("="*50)

    # é…ç½®ï¼ˆåŸºäºåŸè„šæœ¬ä¼˜åŒ–ï¼‰
    config = OptimizedPMCConfig(
        email="1666526339@qq.com",
        api_key="f7f3e5ffa36e0446a4a3c6540d8fa7e72808",
        output_dir="optimized_pmc_test",

        # ä¸‹è½½å‚æ•°
        batch_size=200,  # æ¯æ‰¹200ç¯‡æ–‡ç« 
        disease_batch_size=3,  # æ¯æ‰¹å¤„ç†3ä¸ªç–¾ç—…
        max_records_per_search=10000,

        # è§£æé€‰é¡¹
        save_parsed_json=True,
        save_raw_xml=True,
        parse_detailed_content=True
    )

    print(f"ğŸ“§ é‚®ç®±: {config.email}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"ğŸ“„ æ‰¹æ¬¡å¤§å°: {config.batch_size} ç¯‡/æ‰¹")

    # åŠ è½½ç–¾ç—…åˆ—è¡¨
    disease_file = "/Users/xiong/Documents/github/rare-disease-knowledge-graph/all_rare_disease_names.txt"
    with open(disease_file, 'r', encoding='utf-8') as f:
        all_diseases = [line.strip() for line in f if line.strip()]

    # é€‰æ‹©3ä¸ªç–¾ç—…æµ‹è¯•
    test_diseases = all_diseases[:3]

    print(f"\nğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå¤„ç†å‰ {len(test_diseases)} ä¸ªç–¾ç—…")
    print("ğŸ“‹ ç–¾ç—…åˆ—è¡¨:")
    for i, disease in enumerate(test_diseases, 1):
        print(f"   {i}. {disease}")

    # ç¡®è®¤ç»§ç»­
    response = input(f"\nâ“ ç¡®å®šè¦å¼€å§‹ä¸‹è½½å—ï¼Ÿ(y/N): ").strip().lower()
    if response not in ['y', 'yes']:
        print("âŒ ç”¨æˆ·å–æ¶ˆä¸‹è½½")
        return

    try:
        # åˆå§‹åŒ–ä¸‹è½½å™¨
        downloader = OptimizedPMCDownloader(config)

        # æ‰§è¡Œä¸‹è½½
        results = downloader.process_diseases_batch(test_diseases)

        print(f"\nğŸ‰ ç¤ºä¾‹å®Œæˆï¼")
        print(f"ğŸ’¡ å¦‚éœ€å¤„ç†æ›´å¤šç–¾ç—…ï¼Œè¯·ä¿®æ”¹ main() å‡½æ•°ä¸­çš„ test_diseases")

    except KeyboardInterrupt:
        print(f"\nâš ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()