#!/usr/bin/env python3
"""
PubMedæ‘˜è¦æ‰¹é‡ä¸‹è½½å™¨
ä¸“é—¨é’ˆå¯¹ç½•è§ç–¾ç—…æ–‡çŒ®çš„PubMedæ‘˜è¦ä¸‹è½½
æ”¯æŒå¤§æ‰¹é‡å¤„ç†å’Œé«˜æ•ˆä¸‹è½½ç­–ç•¥
"""

import os
import re
import time
import json
import math
from pathlib import Path
from typing import List, Dict, Optional, Set
from urllib.error import HTTPError, URLError
from http.client import IncompleteRead
from dataclasses import dataclass, asdict
from Bio import Entrez
import xml.etree.ElementTree as ET
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading


@dataclass
class PubMedConfig:
    """PubMedä¸‹è½½é…ç½®"""
    email: str
    api_key: Optional[str] = None
    output_dir: str = "pubmed_data"

    # ä¸‹è½½å‚æ•°
    max_records_per_search: int = 100000  # PubMedå•æ¬¡æœ€å¤§è®°å½•æ•°
    batch_size: int = 1000  # æ¯æ¬¡efetchçš„è®°å½•æ•°
    disease_batch_size: int = 50  # æ¯æ‰¹å¤„ç†çš„ç–¾ç—…æ•°

    # æ§åˆ¶å‚æ•°
    sleep_time: float = 0.34  # æ— API keyæ—¶çš„å»¶è¿Ÿ
    sleep_time_with_key: float = 0.12  # æœ‰API keyæ—¶çš„å»¶è¿Ÿ
    max_retry: int = 3
    request_timeout: int = 30

    # å¹¶å‘æ§åˆ¶
    max_workers: int = 3  # æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°


class PubMedArticle:
    """PubMedæ–‡ç« æ•°æ®ç±»"""

    def __init__(self):
        self.pmid = ""
        self.pmcid = ""  # PMC ID
        self.title = ""
        self.abstract = ""
        self.authors = []
        self.journal = ""
        self.publication_date = {}
        self.doi = ""
        self.mesh_terms = []
        self.publication_types = []
        self.keywords = []
        self.abstract_languages = []
        self.disease = ""  # å…³è”çš„ç½•è§ç–¾ç—…

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        try:
            return asdict(self)
        except Exception:
            # å¦‚æœasdictå¤±è´¥ï¼Œæ‰‹åŠ¨è½¬æ¢
            return {
                'pmid': self.pmid,
                'pmcid': self.pmcid,
                'title': self.title,
                'abstract': self.abstract,
                'authors': self.authors,
                'journal': self.journal,
                'publication_date': self.publication_date,
                'doi': self.doi,
                'mesh_terms': self.mesh_terms,
                'publication_types': self.publication_types,
                'keywords': self.keywords,
                'abstract_languages': self.abstract_languages,
                'disease': self.disease
            }

    def is_valid(self) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦æœ‰æ•ˆ"""
        return bool(self.pmid and self.title)


class PubMedDownloader:
    """PubMedæ‘˜è¦ä¸‹è½½å™¨"""

    def __init__(self, config: PubMedConfig):
        self.config = config
        self.setup_entrez()
        self.setup_directories()
        self.lock = threading.Lock()
        self.processed_pmids: Set[str] = set()  # ç”¨äºå»é‡

    def setup_entrez(self):
        """è®¾ç½®Entrezé…ç½®"""
        Entrez.email = self.config.email
        Entrez.tool = "rare_disease_pubmed_downloader"
        if self.config.api_key:
            Entrez.api_key = self.config.api_key
            print(f"[INFO] ä½¿ç”¨API key: {self.config.api_key[:8]}...")
        else:
            print("[INFO] æœªä½¿ç”¨API keyï¼Œå°†ä½¿ç”¨é»˜è®¤å»¶è¿Ÿ")

    def setup_directories(self):
        """åˆ›å»ºç›®å½•ç»“æ„"""
        self.base_dir = Path(self.config.output_dir)
        self.abstracts_dir = self.base_dir / "abstracts"
        self.metadata_dir = self.base_dir / "metadata"
        self.temp_dir = self.base_dir / "temp"

        for dir_path in [self.base_dir, self.abstracts_dir, self.metadata_dir, self.temp_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)

    def load_disease_list(self, disease_file: str) -> List[str]:
        """åŠ è½½ç½•è§ç–¾ç—…åˆ—è¡¨"""
        with open(disease_file, 'r', encoding='utf-8') as f:
            diseases = [line.strip() for line in f if line.strip()]
        print(f"[INFO] åŠ è½½äº† {len(diseases)} ä¸ªç½•è§ç–¾ç—…")
        return diseases

    def get_sleep_time(self) -> float:
        """è·å–é€‚å½“çš„å»¶è¿Ÿæ—¶é—´"""
        return (self.config.sleep_time_with_key
                if self.config.api_key
                else self.config.sleep_time)

    def safe_search_term(self, disease_name: str) -> str:
        """æ„é€ å®‰å…¨çš„PubMedæ£€ç´¢å¼ - æ‰©å¤§åˆ°å…¨æ–‡æ£€ç´¢"""
        # æ¸…ç†ç–¾ç—…åç§°ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        disease_clean = re.sub(r'[^\w\s\-\.]', ' ', disease_name).strip()

        # é’ˆå¯¹ä¸åŒç±»å‹çš„ç–¾ç—…æ„é€ ä¸åŒçš„æ£€ç´¢ç­–ç•¥
        if 'microdeletion' in disease_clean.lower() or 'microduplication' in disease_clean.lower():
            # æŸ“è‰²ä½“å¾®ç¼ºå¤±/å¾®é‡å¤ç»¼åˆå¾ - ä½¿ç”¨æ›´çµæ´»çš„æ£€ç´¢
            parts = disease_clean.split()
            search_terms = []

            # æå–æŸ“è‰²ä½“åŒºåŸŸ
            chr_region = ''
            for part in parts:
                if 'q' in part and ('.' in part or part.endswith('q')):
                    chr_region = part
                    break

            if chr_region:
                # åŸºäºæŸ“è‰²ä½“åŒºåŸŸçš„æ£€ç´¢ - ä¿æŒTitle/Abstracté™å®šä»¥æé«˜ç²¾ç¡®åº¦
                search_terms.append(f'({chr_region} AND (deletion OR microdeletion OR duplication OR microduplication))[Title/Abstract]')
                search_terms.append(f'("chromosome {chr_region}" AND (deletion OR microdeletion))[Title/Abstract]')

                # æ·»åŠ æ›´ç²¾ç¡®çš„æ£€ç´¢
                search_terms.append(f'("{chr_region} deletion" AND (syndrome OR disorder OR abnormality))[Title/Abstract]')
                search_terms.append(f'("{chr_region} microdeletion" AND (syndrome OR disorder OR abnormality))[Title/Abstract]')

            # æ·»åŠ åŸå§‹æœ¯è¯­çš„å®½æ¾åŒ¹é…
            if 'microdeletion' in disease_clean.lower():
                base_name = disease_clean.replace("microdeletion syndrome", "").strip()
                search_terms.append(f'(microdeletion AND "{base_name}")')
                search_terms.append(f'("{disease_clean}")')
            elif 'microduplication' in disease_clean.lower():
                base_name = disease_clean.replace("microduplication syndrome", "").strip()
                search_terms.append(f'(microduplication AND "{base_name}")')
                search_terms.append(f'("{disease_clean}")')

            # å¦‚æœæ˜¯ç»¼åˆå¾ï¼Œæ·»åŠ ç›¸å…³æœ¯è¯­
            if 'syndrome' in disease_clean.lower():
                base_name = disease_clean.replace('syndrome', '').strip()
                search_terms.append(f'("{base_name}" AND (syndrome OR disorder OR condition))')

            return ' OR '.join(search_terms) if search_terms else disease_clean

        elif 'syndrome' in disease_clean.lower() and len(disease_clean.split()) > 1:
            # å…¶ä»–ç»¼åˆå¾ - ä½¿ç”¨å…³é”®è¯ç»„åˆï¼Œæ‰©å¤§æ£€ç´¢èŒƒå›´
            base_terms = disease_clean.replace('syndrome', '').strip()
            return f'("{base_terms}" AND (syndrome OR disorder OR condition)) OR ("{disease_clean}")'

        elif disease_clean.count(' ') >= 3:
            # å¤æ‚ç–¾ç—…åç§° - ä½¿ç”¨ANDè¿æ¥å…³é”®è¯
            keywords = disease_clean.split()[:3]  # å–å‰3ä¸ªå…³é”®è¯
            return f'({" AND ".join(keywords)}) OR ("{disease_clean}")'

        else:
            # ç®€å•ç–¾ç—…åç§° - ç›´æ¥åŒ¹é…ï¼Œæ‰©å¤§æ£€ç´¢èŒƒå›´
            if ' ' in disease_clean:
                return f'("{disease_clean}") OR ({disease_clean.replace(" ", " AND ")})'
            else:
                return disease_clean

    def retry_call(self, func, *args, **kwargs):
        """é‡è¯•æœºåˆ¶"""
        sleep_time = self.get_sleep_time()

        for attempt in range(self.config.max_retry):
            try:
                return func(*args, **kwargs)
            except (HTTPError, URLError, IncompleteRead) as e:
                if attempt == self.config.max_retry - 1:
                    raise
                wait_time = sleep_time * (attempt + 1) * 2  # æŒ‡æ•°é€€é¿
                print(f"[WARN] ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {wait_time:.1f}s: {e}")
                time.sleep(wait_time)

    def search_pubmed(self, disease: str) -> List[str]:
        """æœç´¢PubMedè·å–PMIDåˆ—è¡¨"""
        search_term = self.safe_search_term(disease)
        print(f"[DEBUG] {disease}: æ£€ç´¢å¼ = {search_term}")

        try:
            # å…ˆè·å–æ€»æ•°
            handle = self.retry_call(
                Entrez.esearch,
                db="pubmed",
                term=search_term,
                retmax=0,
                usehistory="y"
            )
            search_result = Entrez.read(handle)
            handle.close()

            count = int(search_result["Count"])
            if count == 0:
                print(f"[INFO] {disease}: æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®")
                return []

            print(f"[INFO] {disease}: æ‰¾åˆ° {count} ç¯‡ç›¸å…³æ–‡çŒ®")

            # è·å–æ‰€æœ‰PMID
            all_pmids = []
            retmax = min(self.config.max_records_per_search, count)

            # å¦‚æœè®°å½•æ•°è¶…è¿‡å•æ¬¡é™åˆ¶ï¼Œéœ€è¦åˆ†æ‰¹è·å–
            if count > retmax:
                print(f"[INFO] {disease}: æ–‡çŒ®æ•°è¾ƒå¤š({count})ï¼Œå°†åˆ†æ‰¹è·å–")

                # åˆ†æ‰¹æœç´¢
                for retstart in range(0, count, retmax):
                    current_retmax = min(retmax, count - retstart)

                    handle = self.retry_call(
                        Entrez.esearch,
                        db="pubmed",
                        term=search_term,
                        retstart=retstart,
                        retmax=current_retmax,
                        usehistory="y"
                    )
                    result = Entrez.read(handle)
                    handle.close()

                    batch_pmids = result["IdList"]
                    all_pmids.extend(batch_pmids)

                    print(f"[INFO] {disease}: è·å–äº† {len(batch_pmids)} ä¸ªPMID ({len(all_pmids)}/{count})")
                    time.sleep(self.get_sleep_time())
            else:
                # ä¸€æ¬¡æ€§è·å–æ‰€æœ‰PMID
                handle = self.retry_call(
                    Entrez.esearch,
                    db="pubmed",
                    term=search_term,
                    retmax=count,
                    usehistory="y"
                )
                result = Entrez.read(handle)
                handle.close()
                all_pmids = result["IdList"]

            return all_pmids

        except Exception as e:
            print(f"[ERROR] {disease}: æœç´¢å¤±è´¥ - {e}")
            return []

    def fetch_abstracts_batch(self, pmids: List[str], disease: str) -> List[PubMedArticle]:
        """æ‰¹é‡è·å–æ‘˜è¦"""
        if not pmids:
            return []

        print(f"[INFO] {disease}: å¼€å§‹ä¸‹è½½ {len(pmids)} ç¯‡æ‘˜è¦")

        all_articles = []

        # åˆ†æ‰¹è·å–æ‘˜è¦
        for i in range(0, len(pmids), self.config.batch_size):
            batch_pmids = pmids[i:i+self.config.batch_size]
            batch_num = i // self.config.batch_size + 1
            total_batches = math.ceil(len(pmids) / self.config.batch_size)

            print(f"[INFO] {disease}: ä¸‹è½½æ‘˜è¦æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_pmids)} ç¯‡)")

            try:
                # ä½¿ç”¨postæ–¹æ³•è·å–æ‘˜è¦
                handle = self.retry_call(
                    Entrez.efetch,
                    db="pubmed",
                    id=batch_pmids,
                    rettype="xml",
                    retmode="xml"
                )

                xml_content = handle.read()
                handle.close()

                # è§£æXML
                articles = self.parse_pubmed_xml(xml_content, disease)
                all_articles.extend(articles)

                print(f"[OK] {disease}: æ‰¹æ¬¡ {batch_num} æˆåŠŸè§£æ {len(articles)} ç¯‡æ–‡ç« ")

            except Exception as e:
                print(f"[ERROR] {disease}: æ‰¹æ¬¡ {batch_num} ä¸‹è½½å¤±è´¥ - {e}")

            time.sleep(self.get_sleep_time())

        return all_articles

    def parse_pubmed_xml(self, xml_content, disease: str) -> List[PubMedArticle]:
        """è§£æPubMed XMLæ•°æ®"""
        try:
            if isinstance(xml_content, bytes):
                xml_content = xml_content.decode('utf-8')

            root = ET.fromstring(xml_content)
            articles = []

            for article_elem in root.findall('.//PubmedArticle'):
                try:
                    article = PubMedArticle()
                    article.disease = disease

                    # è§£æå„ä¸ªå­—æ®µ
                    article.pmid = self._get_text(article_elem, './/PMID')
                    article.pmcid = self._get_text(article_elem, './/ArticleId[@IdType="pmc"]')
                    # ç¡®ä¿PMCIDæ ¼å¼æ­£ç¡®ï¼ˆç§»é™¤PMCå‰ç¼€ä¸­çš„æ•°å­—ï¼‰
                    if article.pmcid and article.pmcid.startswith('PMC'):
                        article.pmcid = article.pmcid  # ä¿æŒå®Œæ•´æ ¼å¼å¦‚ "PMC123456"

                    article.title = self._get_text(article_elem, './/ArticleTitle')
                    article.abstract = self._parse_abstract(article_elem)
                    article.authors = self._parse_authors(article_elem)
                    article.journal = self._get_text(article_elem, './/JournalTitle')
                    article.publication_date = self._parse_publication_date(article_elem)
                    article.doi = self._get_text(article_elem, './/ArticleId[@IdType="doi"]')
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°DOIï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„DOIæ ¼å¼
                    if not article.doi:
                        article.doi = self._get_text(article_elem, './/ELocationID[@EIdType="doi"]')
                    article.mesh_terms = self._parse_mesh_terms(article_elem)
                    article.publication_types = self._parse_publication_types(article_elem)
                    article.keywords = self._parse_keywords(article_elem)
                    article.abstract_languages = self._parse_languages(article_elem)

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ•ˆä¸”æœªé‡å¤
                    if article.is_valid() and article.pmid not in self.processed_pmids:
                        articles.append(article)
                        self.processed_pmids.add(article.pmid)

                except Exception as e:
                    print(f"[WARN] è§£ææ–‡ç« å¤±è´¥: {e}")
                    continue

            return articles

        except ET.ParseError as e:
            print(f"[ERROR] XMLè§£æå¤±è´¥: {e}")
            return []

    def _get_text(self, element, xpath: str) -> str:
        """å®‰å…¨è·å–XMLå…ƒç´ æ–‡æœ¬"""
        elem = element.find(xpath)
        return elem.text.strip() if elem is not None and elem.text else ""

    def _parse_abstract(self, article_elem) -> str:
        """è§£ææ‘˜è¦æ–‡æœ¬"""
        abstract_texts = []
        for abs_text in article_elem.findall('.//AbstractText'):
            if abs_text.text:
                abstract_texts.append(abs_text.text.strip())

        return ' '.join(abstract_texts)

    def _parse_authors(self, article_elem) -> List[str]:
        """è§£æä½œè€…ä¿¡æ¯"""
        authors = []
        for author in article_elem.findall('.//Author'):
            last_name = self._get_text(author, './/LastName')
            fore_name = self._get_text(author, './/ForeName')
            initials = self._get_text(author, './/Initials')

            if last_name:
                author_name = f"{last_name} {fore_name}".strip()
                if not author_name or author_name == last_name:
                    author_name = f"{last_name} {initials}".strip()
                authors.append(author_name)

        return authors[:10]  # é™åˆ¶ä½œè€…æ•°é‡

    def _parse_publication_date(self, article_elem) -> Dict:
        """è§£æå‘è¡¨æ—¥æœŸ"""
        pub_date = article_elem.find('.//PubDate')
        if pub_date is not None:
            year = self._get_text(pub_date, './/Year')
            month = self._get_text(pub_date, './/Month')
            day = self._get_text(pub_date, './/Day')

            # æ ¼å¼åŒ–æ—¥æœŸ
            date_parts = []
            if year:
                date_parts.append(year)
            if month:
                date_parts.append(month.zfill(2))
            if day:
                date_parts.append(day.zfill(2))

            formatted_date = '-'.join(date_parts) if date_parts else year

            return {
                'year': year,
                'month': month,
                'day': day,
                'formatted': formatted_date
            }
        return {}

    def _parse_mesh_terms(self, article_elem) -> List[str]:
        """è§£æMeSHæœ¯è¯­"""
        mesh_terms = []
        for mesh in article_elem.findall('.//MeshHeading'):
            descriptor = self._get_text(mesh, './/DescriptorName')
            if descriptor:
                mesh_terms.append(descriptor)
        return mesh_terms

    def _parse_publication_types(self, article_elem) -> List[str]:
        """è§£æå‘è¡¨ç±»å‹"""
        pub_types = []
        for pub_type in article_elem.findall('.//PublicationType'):
            if pub_type.text:
                pub_types.append(pub_type.text.strip())
        return pub_types

    def _parse_keywords(self, article_elem) -> List[str]:
        """è§£æå…³é”®è¯"""
        keywords = []
        for keyword in article_elem.findall('.//Keyword'):
            if keyword.text:
                keywords.append(keyword.text.strip())
        return keywords

    def _parse_languages(self, article_elem) -> List[str]:
        """è§£æè¯­è¨€ä¿¡æ¯"""
        languages = []
        for lang in article_elem.findall('.//Abstract/AbstractText[@Language]'):
            lang_code = lang.get('Language')
            if lang_code:
                languages.append(lang_code)
        return list(set(languages))

    def save_articles(self, articles: List[PubMedArticle], disease: str):
        """ä¿å­˜æ–‡ç« æ•°æ®"""
        if not articles:
            return

        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        safe_disease_name = re.sub(r'[^\w\-\.]+', '_', disease)[:50]
        timestamp = time.strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜JSONæ–‡ä»¶
        json_file = self.abstracts_dir / f"{safe_disease_name}_{timestamp}.json"

        data = {
            'disease': disease,
            'search_timestamp': timestamp,
            'total_articles': len(articles),
            'articles': [article.to_dict() for article in articles]
        }

        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"[OK] ä¿å­˜æ‘˜è¦: {json_file} ({len(articles)} ç¯‡)")

        # ä¿å­˜CSVæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        self.save_articles_csv(articles, disease, safe_disease_name, timestamp)

    def save_articles_csv(self, articles: List[PubMedArticle], disease: str, safe_name: str, timestamp: str):
        """ä¿å­˜ä¸ºCSVæ ¼å¼"""
        import csv

        csv_file = self.abstracts_dir / f"{safe_name}_{timestamp}.csv"

        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            # å†™å…¥æ ‡é¢˜è¡Œ
            writer.writerow([
                'PMID', 'PMCID', 'Title', 'Abstract', 'Authors', 'Journal',
                'Publication Year', 'DOI', 'MeSH Terms', 'Publication Types',
                'Keywords', 'Languages', 'Disease'
            ])

            # å†™å…¥æ•°æ®
            for article in articles:
                writer.writerow([
                    article.pmid,
                    article.pmcid,
                    article.title,
                    article.abstract[:1000] + '...' if len(article.abstract) > 1000 else article.abstract,
                    '; '.join(article.authors),
                    article.journal,
                    article.publication_date.get('year', ''),
                    article.doi,
                    '; '.join(article.mesh_terms),
                    '; '.join(article.publication_types),
                    '; '.join(article.keywords),
                    '; '.join(article.abstract_languages),
                    article.disease
                ])

        print(f"[OK] ä¿å­˜CSV: {csv_file}")

    def process_single_disease(self, disease: str) -> Dict:
        """å¤„ç†å•ä¸ªç–¾ç—…"""
        print(f"\n{'='*60}")
        print(f"ğŸ”¬ å¤„ç†ç–¾ç—…: {disease}")
        print(f"{'='*60}")

        result = {
            'disease': disease,
            'success': False,
            'pmids_found': 0,
            'articles_downloaded': 0,
            'error': None,
            'processing_time': 0
        }

        start_time = time.time()

        try:
            # æœç´¢PubMed
            pmids = self.search_pubmed(disease)
            result['pmids_found'] = len(pmids)

            if not pmids:
                result['success'] = True  # æ²¡æ‰¾åˆ°æ–‡çŒ®ä¹Ÿç®—æˆåŠŸ
                print(f"[INFO] {disease}: æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®")
                return result

            # ä¸‹è½½æ‘˜è¦
            articles = self.fetch_abstracts_batch(pmids, disease)
            result['articles_downloaded'] = len(articles)

            # ä¿å­˜æ•°æ®
            if articles:
                self.save_articles(articles, disease)

            result['success'] = True
            print(f"[OK] {disease}: å®Œæˆï¼Œè·å¾— {len(articles)} ç¯‡æ‘˜è¦")

        except Exception as e:
            result['error'] = str(e)
            print(f"[ERROR] {disease}: å¤„ç†å¤±è´¥ - {e}")

        finally:
            result['processing_time'] = time.time() - start_time

        return result

    def process_diseases_batch(self, diseases: List[str]) -> List[Dict]:
        """æ‰¹é‡å¤„ç†ç–¾ç—…"""
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(diseases)} ä¸ªç–¾ç—…")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.base_dir}")
        print(f"ğŸ§µ å¹¶å‘çº¿ç¨‹æ•°: {self.config.max_workers}")

        results = []

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_disease = {
                executor.submit(self.process_single_disease, disease): disease
                for disease in diseases
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for i, future in enumerate(as_completed(future_to_disease), 1):
                disease = future_to_disease[future]
                print(f"\nğŸ“‹ è¿›åº¦: {i}/{len(diseases)} - {disease}")

                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    print(f"[ERROR] {disease}: ä»»åŠ¡æ‰§è¡Œå¤±è´¥ - {e}")
                    results.append({
                        'disease': disease,
                        'success': False,
                        'error': f"Task execution failed: {e}",
                        'pmids_found': 0,
                        'articles_downloaded': 0,
                        'processing_time': 0
                    })

        # ä¿å­˜æ‰¹å¤„ç†ç»“æœ
        self.save_batch_results(results)
        self.print_batch_summary(results)

        return results

    def save_batch_results(self, results: List[Dict]):
        """ä¿å­˜æ‰¹å¤„ç†ç»“æœ"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        results_file = self.metadata_dir / f"batch_results_{timestamp}.json"

        summary = {
            'timestamp': timestamp,
            'total_diseases': len(results),
            'successful_diseases': sum(1 for r in results if r['success']),
            'total_pmids': sum(r['pmids_found'] for r in results),
            'total_articles': sum(r['articles_downloaded'] for r in results),
            'total_processing_time': sum(r['processing_time'] for r in results),
            'failed_diseases': [r['disease'] for r in results if not r['success']],
            'detailed_results': results
        }

        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        print(f"\n[INFO] æ‰¹å¤„ç†ç»“æœå·²ä¿å­˜: {results_file}")

    def print_batch_summary(self, results: List[Dict]):
        """æ‰“å°æ‰¹å¤„ç†æ€»ç»“"""
        print("\n" + "="*80)
        print("ğŸ“Š PubMedæ‰¹é‡ä¸‹è½½å®Œæˆæ€»ç»“")
        print("="*80)

        successful = [r for r in results if r['success']]
        failed = [r for r in results if not r['success']]

        print(f"ğŸ“… å¤„ç†æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ”¬ æ€»ç–¾ç—…æ•°: {len(results)}")
        print(f"âœ… æˆåŠŸå¤„ç†: {len(successful)}")
        print(f"âŒ å¤„ç†å¤±è´¥: {len(failed)}")

        if successful:
            total_pmids = sum(r['pmids_found'] for r in successful)
            total_articles = sum(r['articles_downloaded'] for r in successful)
            total_time = sum(r['processing_time'] for r in successful)

            print(f"ğŸ“Š æ‰¾åˆ°PMID: {total_pmids}")
            print(f"ğŸ“„ ä¸‹è½½æ‘˜è¦: {total_articles}")
            print(f"â±ï¸  æ€»ç”¨æ—¶: {total_time:.1f} ç§’")
            print(f"âš¡ å¹³å‡é€Ÿåº¦: {total_articles/max(total_time, 1):.1f} ç¯‡/ç§’")

        if failed:
            print(f"\nâŒ å¤±è´¥çš„ç–¾ç—…:")
            for result in failed[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"   - {result['disease']}: {result.get('error', 'Unknown error')}")
            if len(failed) > 10:
                print(f"   ... è¿˜æœ‰ {len(failed) - 10} ä¸ªå¤±è´¥ç–¾ç—…")

        print(f"\nğŸ“ æ•°æ®ä¿å­˜åœ¨: {self.base_dir}")
        print(f"ğŸ“‹ è¯¦ç»†ç»“æœåœ¨: {self.metadata_dir}")
        print("="*80)


def main():
    """ä¸»å‡½æ•°"""

    # é…ç½®
    config = PubMedConfig(
        email="your_email@example.com",  # è¯·æ›¿æ¢ä¸ºä½ çš„é‚®ç®±
        api_key=None,  # å¦‚æœ‰NCBI API keyå¯å¡«å…¥ï¼Œå¯å¤§å¹…æé«˜ä¸‹è½½é€Ÿåº¦
        output_dir="pubmed_data",
        disease_batch_size=50,  # æ¯æ‰¹å¤„ç†50ä¸ªç–¾ç—…
        batch_size=1000,  # æ¯æ¬¡efetchè·å–1000æ¡è®°å½•
        max_workers=3,  # 3ä¸ªå¹¶å‘çº¿ç¨‹
        sleep_time=0.34,  # æ— API keyæ—¶çš„å»¶è¿Ÿ
        sleep_time_with_key=0.12  # æœ‰API keyæ—¶çš„å»¶è¿Ÿ
    )

    # åˆå§‹åŒ–ä¸‹è½½å™¨
    downloader = PubMedDownloader(config)

    # åŠ è½½ç–¾ç—…åˆ—è¡¨
    disease_file = "/Users/xiong/Documents/github/rare-disease-knowledge-graph/all_rare_disease_names.txt"
    all_diseases = downloader.load_disease_list(disease_file)

    # ç¤ºä¾‹ï¼šå¤„ç†å‰10ä¸ªç–¾ç—…ä½œä¸ºæµ‹è¯•
    test_diseases = all_diseases[:10]

    print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šå¤„ç†å‰ {len(test_diseases)} ä¸ªç–¾ç—…")
    print(f"ğŸ“‹ ç–¾ç—…åˆ—è¡¨: {', '.join(test_diseases[:3])}...")

    # æ‰§è¡Œä¸‹è½½
    results = downloader.process_diseases_batch(test_diseases)

    print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ’¡ å¦‚éœ€å¤„ç†å…¨éƒ¨{len(all_diseases)}ä¸ªç–¾ç—…ï¼Œè¯·ä¿®æ”¹ main() å‡½æ•°ä¸­çš„ test_diseases")
    print(f"ğŸ’¡ å»ºè®®è·å–NCBI API keyä»¥æé«˜ä¸‹è½½é€Ÿåº¦ï¼šhttps://www.ncbi.nlm.nih.gov/account/")


if __name__ == "__main__":
    main()