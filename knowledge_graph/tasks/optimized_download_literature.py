#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆç½•è§ç–¾ç—…æ–‡çŒ®ä¸‹è½½è„šæœ¬
é‡‡ç”¨ä¸¤é˜¶æ®µç­–ç•¥ï¼š
1. é˜¶æ®µä¸€ï¼šæ”¶é›†æ‰€æœ‰ç–¾ç—…çš„æ–‡çŒ®IDï¼Œå»é‡
2. é˜¶æ®µäºŒï¼šæ‰¹é‡ä¸‹è½½å»é‡åçš„æ–‡çŒ®ï¼Œå»ºç«‹ç–¾ç—…-æ–‡çŒ®æ˜ å°„å…³ç³»
"""

import os
import sys
import time
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Set, Tuple
from collections import defaultdict
from dataclasses import dataclass, asdict
import pickle

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
src_dir = project_root / "src"
sys.path.insert(0, str(src_dir))

from literature_downloader import OptimizedPMCDownloader, OptimizedPMCConfig, PubMedDownloader, PubMedConfig


@dataclass
class DiseaseLiteratureInfo:
    """ç–¾ç—…æ–‡çŒ®ä¿¡æ¯"""
    disease: str
    search_terms: List[str]
    pmc_ids: List[str]
    pmids: List[str]
    pmc_count: int
    pmid_count: int
    processing_time: float
    success: bool
    error: Optional[str] = None


@dataclass
class LiteratureMetadata:
    """æ–‡çŒ®å…ƒæ•°æ®"""
    pmc_id: str = ""
    pmid: str = ""
    title: str = ""
    authors: List[str] = None
    journal: str = ""
    publication_date: str = ""
    abstract: str = ""
    doi: str = ""
    related_diseases: List[str] = None

    def __post_init__(self):
        if self.authors is None:
            self.authors = []
        if self.related_diseases is None:
            self.related_diseases = []


class OptimizedLiteratureDownloader:
    """ä¼˜åŒ–ç‰ˆæ–‡çŒ®ä¸‹è½½å™¨"""

    def __init__(self, download_mode: str = "pmc_only"):
        """
        åˆå§‹åŒ–é…ç½®

        Args:
            download_mode: ä¸‹è½½æ¨¡å¼
                - "pmc_only": ä»…ä¸‹è½½PMCå…¨æ–‡
                - "pubmed_only": ä»…ä¸‹è½½PubMedæ‘˜è¦
                - "both": åŒæ—¶ä¸‹è½½PubMedæ‘˜è¦å’ŒPMCå…¨æ–‡
        """
        self.email = "1666526339@qq.com"
        self.api_key = "f7f3e5ffa36e0446a4a3c6540d8fa7e72808"
        self.download_mode = download_mode

        # è¾“å‡ºç›®å½•
        self.base_output_dir = project_root / "knowledge_graph" / "data" / "literature"
        self.base_output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ä¸‹è½½å™¨
        self.init_downloaders()

        # æ•°æ®å­˜å‚¨
        self.disease_literature_mapping: Dict[str, DiseaseLiteratureInfo] = {}
        self.unique_pmc_ids: Set[str] = set()
        self.unique_pmids: Set[str] = set()
        self.literature_disease_mapping: Dict[str, List[str]] = defaultdict(list)
        self.literature_metadata: Dict[str, LiteratureMetadata] = {}

        # æ–­ç‚¹ç»­ä¼ æ–‡ä»¶è·¯å¾„
        self.progress_file = self.base_output_dir / "progress_state.pkl"
        self.disease_pmc_mapping_file = self.base_output_dir / "disease_pmc_mapping.json"

    def init_downloaders(self):
        """åˆå§‹åŒ–ä¸‹è½½å™¨"""
        # PMCä¸‹è½½å™¨é…ç½®
        if self.download_mode in ["pmc_only", "both"]:
            self.pmc_config = OptimizedPMCConfig(
                email=self.email,
                api_key=self.api_key,
                output_dir=str(self.base_output_dir / "PMC_full_text"),
                batch_size=500,  # å¢å¤§æ‰¹æ¬¡æé«˜æ•ˆç‡
                disease_batch_size=50,  # å¢å¤§ç–¾ç—…æ‰¹æ¬¡
                max_records_per_search=100000,
                sleep_time=0.34,
                sleep_time_with_key=0.12,
                max_retry=3,
                save_parsed_json=True,
                save_raw_xml=True,
                parse_detailed_content=True
            )
            self.pmc_downloader = OptimizedPMCDownloader(self.pmc_config)

        # PubMedä¸‹è½½å™¨é…ç½®
        if self.download_mode in ["pubmed_only", "both"]:
            self.pubmed_config = PubMedConfig(
                email=self.email,
                api_key=self.api_key,
                output_dir=str(self.base_output_dir / "PubMed_abstracts"),
                max_records_per_search=100000,
                batch_size=1000,
                disease_batch_size=50,
                sleep_time=0.34,
                sleep_time_with_key=0.12,
                max_retry=3,
                request_timeout=30,
                max_workers=3
            )
            self.pubmed_downloader = PubMedDownloader(self.pubmed_config)

    def load_progress_state(self) -> Dict:
        """åŠ è½½è¿›åº¦çŠ¶æ€"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'rb') as f:
                    state = pickle.load(f)
                print(f"ğŸ“‚ å‘ç°æœ‰æ–­ç‚¹æ–‡ä»¶ï¼Œå·²å¤„ç† {len(state.get('processed_diseases', []))} ä¸ªç–¾ç—…")
                return state
            except Exception as e:
                print(f"âš ï¸  æ–­ç‚¹æ–‡ä»¶æŸåï¼Œé‡æ–°å¼€å§‹: {e}")
        return {}

    def save_progress_state(self, processed_diseases: List[str], current_disease_index: int):
        """ä¿å­˜è¿›åº¦çŠ¶æ€"""
        try:
            state = {
                'processed_diseases': processed_diseases,
                'current_disease_index': current_disease_index,
                'disease_literature_mapping': self.disease_literature_mapping,
                'unique_pmc_ids': self.unique_pmc_ids,
                'unique_pmids': self.unique_pmids,
                'literature_disease_mapping': dict(self.literature_disease_mapping),
                'timestamp': datetime.now().isoformat()
            }

            with open(self.progress_file, 'wb') as f:
                pickle.dump(state, f)

        except Exception as e:
            print(f"âš ï¸  ä¿å­˜è¿›åº¦å¤±è´¥: {e}")

    def save_disease_pmc_mapping(self):
        """ä¿å­˜ç–¾ç—…-PMC IDæ˜ å°„å…³ç³»"""
        mapping_data = {}
        for disease, info in self.disease_literature_mapping.items():
            if info.success and info.pmc_ids:
                mapping_data[disease] = {
                    'pmc_ids': info.pmc_ids,
                    'pmc_count': len(info.pmc_ids),
                    'pmids': info.pmids,
                    'pmid_count': len(info.pmids),
                    'processing_time': info.processing_time,
                    'last_updated': datetime.now().isoformat()
                }

        try:
            with open(self.disease_pmc_mapping_file, 'w', encoding='utf-8') as f:
                json.dump(mapping_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“‹ ç–¾ç—…-PMCæ˜ å°„å…³ç³»å·²ä¿å­˜: {self.disease_pmc_mapping_file}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç–¾ç—…-PMCæ˜ å°„å¤±è´¥: {e}")

    def load_disease_list(self) -> List[str]:
        """åŠ è½½ç½•è§ç–¾ç—…åˆ—è¡¨"""
        disease_file = project_root / "all_rare_disease_names.txt"
        print(f"ğŸ“‹ åŠ è½½ç–¾ç—…åˆ—è¡¨: {disease_file}")

        with open(disease_file, 'r', encoding='utf-8') as f:
            diseases = [line.strip() for line in f if line.strip()]

        print(f"âœ… åŠ è½½äº† {len(diseases)} ä¸ªç½•è§ç–¾ç—…")
        return diseases

    def stage_one_collect_literature_ids(self, diseases: List[str], max_diseases: Optional[int] = None):
        """é˜¶æ®µä¸€ï¼šæ”¶é›†æ‰€æœ‰ç–¾ç—…çš„æ–‡çŒ®IDï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰"""
        if max_diseases:
            diseases = diseases[:max_diseases]

        # åŠ è½½è¿›åº¦çŠ¶æ€
        progress_state = self.load_progress_state()
        processed_diseases = set(progress_state.get('processed_diseases', []))

        # æ¢å¤ä¹‹å‰çš„çŠ¶æ€
        if progress_state:
            self.disease_literature_mapping = progress_state.get('disease_literature_mapping', {})
            self.unique_pmc_ids = set(progress_state.get('unique_pmc_ids', []))
            self.unique_pmids = set(progress_state.get('unique_pmids', []))
            self.literature_disease_mapping = defaultdict(list, progress_state.get('literature_disease_mapping', {}))

        # è¿‡æ»¤æœªå¤„ç†çš„ç–¾ç—…
        remaining_diseases = [d for d in diseases if d not in processed_diseases]

        if not remaining_diseases:
            print("âœ… æ‰€æœ‰ç–¾ç—…å·²å¤„ç†å®Œæ¯•ï¼")
            return

        print(f"\nğŸš€ é˜¶æ®µä¸€ï¼šæ”¶é›†æ–‡çŒ®IDï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰")
        print(f"ğŸ“Š æ€»ç–¾ç—…æ•°: {len(diseases)}, å·²å¤„ç†: {len(processed_diseases)}, å‰©ä½™: {len(remaining_diseases)}")
        print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80)

        start_time = time.time()
        successful_collections = len([d for d in processed_diseases if hasattr(self.disease_literature_mapping.get(d), 'success') and self.disease_literature_mapping.get(d).success])
        failed_collections = len(processed_diseases) - successful_collections

        for i, disease in enumerate(remaining_diseases, 1):
            actual_index = len(processed_diseases) + i
            print(f"\nğŸ“‹ è¿›åº¦: {actual_index}/{len(diseases)} - {disease[:80]}...")

            disease_info = self.collect_single_disease_literature(disease)
            self.disease_literature_mapping[disease] = disease_info

            if disease_info.success:
                successful_collections += 1

                # æ·»åŠ åˆ°å»é‡é›†åˆ
                self.unique_pmc_ids.update(disease_info.pmc_ids)
                self.unique_pmids.update(disease_info.pmids)

                # å»ºç«‹æ–‡çŒ®-ç–¾ç—…æ˜ å°„
                for pmc_id in disease_info.pmc_ids:
                    self.literature_disease_mapping[pmc_id].append(disease)
                for pmid in disease_info.pmids:
                    self.literature_disease_mapping[pmid].append(disease)

            else:
                failed_collections += 1

            # æ›´æ–°å·²å¤„ç†ç–¾ç—…åˆ—è¡¨
            processed_diseases.add(disease)

            # æ¯20ä¸ªç–¾ç—…ä¿å­˜ä¸€æ¬¡è¿›åº¦å¹¶æ˜¾ç¤ºè¿›åº¦
            if i % 20 == 0:
                self.save_progress_state(list(processed_diseases), actual_index)
                self.show_collection_progress(actual_index, successful_collections, failed_collections)
                # ä¿å­˜ç–¾ç—…-PMCæ˜ å°„å…³ç³»
                self.save_disease_pmc_mapping()

            # çŸ­æš‚å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.3)

        # æœ€ç»ˆä¿å­˜è¿›åº¦
        self.save_progress_state(list(processed_diseases), len(diseases))
        self.save_disease_pmc_mapping()

        collection_time = time.time() - start_time

        print(f"\nâœ… é˜¶æ®µä¸€å®Œæˆï¼")
        print(f"â° ç”¨æ—¶: {collection_time:.1f} ç§’")
        print(f"ğŸ“Š æˆåŠŸæ”¶é›†: {successful_collections} ä¸ªç–¾ç—…")
        print(f"âŒ æ”¶é›†å¤±è´¥: {failed_collections} ä¸ªç–¾ç—…")
        print(f"ğŸ” å»é‡å‰ PMC IDs: {sum(len(info.pmc_ids) for info in self.disease_literature_mapping.values())}")
        print(f"ğŸ” å»é‡å‰ PubMed IDs: {sum(len(info.pmids) for info in self.disease_literature_mapping.values())}")
        print(f"âœ¨ å»é‡å PMC IDs: {len(self.unique_pmc_ids)}")
        print(f"âœ¨ å»é‡å PubMed IDs: {len(self.unique_pmids)}")

        # è®¡ç®—å»é‡æ•ˆæœ
        original_pmc = sum(len(info.pmc_ids) for info in self.disease_literature_mapping.values())
        original_pmid = sum(len(info.pmids) for info in self.disease_literature_mapping.values())

        if original_pmc > 0:
            pmc_reduction = (original_pmc - len(self.unique_pmc_ids)) / original_pmc * 100
            print(f"ğŸ“ˆ PMC IDå»é‡ç‡: {pmc_reduction:.1f}%")

        if original_pmid > 0:
            pmid_reduction = (original_pmid - len(self.unique_pmids)) / original_pmid * 100
            print(f"ğŸ“ˆ PubMed IDå»é‡ç‡: {pmid_reduction:.1f}%")

    def collect_single_disease_literature(self, disease: str) -> DiseaseLiteratureInfo:
        """æ”¶é›†å•ä¸ªç–¾ç—…çš„æ–‡çŒ®ID"""
        disease_info = DiseaseLiteratureInfo(
            disease=disease,
            search_terms=[disease],  # å¯ä»¥æ‰©å±•ä¸ºå¤šä¸ªæ£€ç´¢è¯
            pmc_ids=[],
            pmids=[],
            pmc_count=0,
            pmid_count=0,
            processing_time=0,
            success=False
        )

        start_time = time.time()

        try:
            # PMCæ£€ç´¢
            if self.download_mode in ["pmc_only", "both"]:
                print(f"   ğŸ” æ£€ç´¢PMCå…¨æ–‡...")
                # ä½¿ç”¨æ–°çš„IDæ”¶é›†æ–¹æ³•
                pmc_ids = self.pmc_downloader.collect_pmc_ids_only(disease)
                disease_info.pmc_ids = pmc_ids
                disease_info.pmc_count = len(pmc_ids)

                if pmc_ids:
                    print(f"   ğŸ“š æ‰¾åˆ° {len(pmc_ids)} ä¸ªå»é‡åPMC ID")

            # PubMedæ£€ç´¢
            if self.download_mode in ["pubmed_only", "both"]:
                print(f"   ğŸ” æ£€ç´¢PubMedæ‘˜è¦...")
                # ä½¿ç”¨PubMedçš„IDæ”¶é›†æ–¹æ³•
                try:
                    pmids = self.pubmed_downloader.search_pubmed(disease)
                    disease_info.pmids = pmids
                    disease_info.pmid_count = len(pmids)

                    if pmids:
                        print(f"   ğŸ“„ æ‰¾åˆ° {len(pmids)} ä¸ªå»é‡åPMID")

                except Exception as e:
                    print(f"   âŒ PubMedæ£€ç´¢å¤±è´¥: {e}")

            disease_info.success = True

        except Exception as e:
            print(f"   âŒ æ£€ç´¢å¤±è´¥: {e}")
            disease_info.error = str(e)

        disease_info.processing_time = time.time() - start_time
        return disease_info

    def show_collection_progress(self, processed_count: int, successful: int, failed: int):
        """æ˜¾ç¤ºæ”¶é›†è¿›åº¦"""
        success_rate = (successful / processed_count) * 100 if processed_count > 0 else 0

        print(f"\nğŸ“Š æ”¶é›†è¿›åº¦æ‘˜è¦ (å¤„ç†äº† {processed_count} ä¸ªç–¾ç—…):")
        print(f"   âœ… æˆåŠŸæ”¶é›†: {successful} ({success_rate:.1f}%)")
        print(f"   âŒ æ”¶é›†å¤±è´¥: {failed}")

        # æ˜¾ç¤ºå½“å‰çš„å»é‡ç»Ÿè®¡
        current_pmc = len(self.unique_pmc_ids)
        current_pmid = len(self.unique_pmids)
        print(f"   ğŸ“š å½“å‰å»é‡PMC IDs: {current_pmc}")
        print(f"   ğŸ“„ å½“å‰å»é‡PubMed IDs: {current_pmid}")

    def stage_two_batch_download(self):
        """é˜¶æ®µäºŒï¼šæ‰¹é‡ä¸‹è½½å»é‡åçš„æ–‡çŒ®"""
        print(f"\nğŸš€ é˜¶æ®µäºŒï¼šæ‰¹é‡ä¸‹è½½å»é‡åçš„æ–‡çŒ®")
        print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*80)

        start_time = time.time()

        # PMCæ‰¹é‡ä¸‹è½½
        if self.download_mode in ["pmc_only", "both"] and self.unique_pmc_ids:
            print(f"\nğŸ“š å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(self.unique_pmc_ids)} ä¸ªPMCå…¨æ–‡...")
            self.batch_download_pmc_articles()

        # PubMedæ‰¹é‡ä¸‹è½½
        if self.download_mode in ["pubmed_only", "both"] and self.unique_pmids:
            print(f"\nğŸ“„ å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(self.unique_pmids)} ä¸ªPubMedæ‘˜è¦...")
            self.batch_download_pubmed_abstracts()

        download_time = time.time() - start_time

        print(f"\nâœ… é˜¶æ®µäºŒå®Œæˆï¼")
        print(f"â° ç”¨æ—¶: {download_time:.1f} ç§’")

    def batch_download_pmc_articles(self):
        """æ‰¹é‡ä¸‹è½½PMCæ–‡ç« """
        if not self.unique_pmc_ids:
            print("   â„¹ï¸  æ²¡æœ‰PMCæ–‡ç« éœ€è¦ä¸‹è½½")
            return

        # å°†PMC IDè½¬æ¢ä¸ºåˆ—è¡¨å¹¶åˆ†æ‰¹å¤„ç†
        pmc_id_list = list(self.unique_pmc_ids)
        batch_size = self.pmc_config.batch_size

        total_batches = (len(pmc_id_list) + batch_size - 1) // batch_size
        successful_downloads = 0
        failed_downloads = 0
        all_pmc_articles = []

        print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   ğŸ“¦ æ€»æ‰¹æ¬¡æ•°: {total_batches}")

        for i in range(0, len(pmc_id_list), batch_size):
            batch_ids = pmc_id_list[i:i + batch_size]
            batch_num = (i // batch_size) + 1

            print(f"   ğŸ“¥ ä¸‹è½½æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_ids)} ç¯‡)...")

            try:
                # è¿™é‡Œä½¿ç”¨ç°æœ‰çš„æ‰¹é‡ä¸‹è½½é€»è¾‘
                # éœ€è¦é€‚é…ç°æœ‰çš„ä¸‹è½½å™¨æ¥å£
                batch_articles = self.download_pmc_batch(batch_ids, batch_num)
                if batch_articles:
                    all_pmc_articles.extend(batch_articles)
                successful_downloads += len(batch_ids)

            except Exception as e:
                print(f"   âŒ æ‰¹æ¬¡ {batch_num} ä¸‹è½½å¤±è´¥: {e}")
                failed_downloads += len(batch_ids)

            # è¯·æ±‚é—´éš”
            time.sleep(self.pmc_downloader.get_sleep_time())

        print(f"   âœ… PMCä¸‹è½½å®Œæˆ: æˆåŠŸ {successful_downloads}, å¤±è´¥ {failed_downloads}")

        # ä¿å­˜åˆå¹¶çš„PMCæ•°æ®ä¸ºCSV
        if all_pmc_articles:
            self.save_pmc_csv_data(all_pmc_articles)

    def download_pmc_batch(self, pmc_ids: List[str], batch_num: int) -> List[Dict]:
        """ä¸‹è½½PMCæ‰¹æ¬¡"""
        # ä½¿ç”¨ç°æœ‰çš„PMCä¸‹è½½å™¨é€»è¾‘
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„ä¸‹è½½å™¨æ¥å£è¿›è¡Œé€‚é…

        # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
        batch_dir = Path(self.pmc_config.output_dir) / "batch_downloads"
        batch_dir.mkdir(exist_ok=True)

        # æ„å»ºæ‰¹æ¬¡æ–‡ä»¶å
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        batch_filename = f"optimized_batch_{batch_num:05d}_{timestamp}.xml"
        batch_file = batch_dir / batch_filename

        batch_articles = []

        try:
            # ä½¿ç”¨ç°æœ‰çš„ä¸‹è½½é€»è¾‘
            xml_text = self.pmc_downloader._safe_fetch_with_retry(pmc_ids)

            # ä¿å­˜XMLæ–‡ä»¶
            with open(batch_file, 'w', encoding='utf-8') as f:
                f.write(xml_text)

            print(f"   âœ… ä¿å­˜æ‰¹æ¬¡æ–‡ä»¶: {batch_filename}")

            # è§£æå¹¶ä¿å­˜å…ƒæ•°æ®
            if self.pmc_config.parse_detailed_content:
                batch_articles = self.parse_batch_metadata(xml_text, pmc_ids, batch_num)

        except Exception as e:
            print(f"   âŒ æ‰¹æ¬¡ä¸‹è½½å¤±è´¥: {e}")
            raise

        return batch_articles

    def parse_batch_metadata(self, xml_text: str, pmc_ids: List[str], batch_num: int) -> List[Dict]:
        """è§£ææ‰¹æ¬¡å…ƒæ•°æ®"""
        enhanced_articles = []
        try:
            # ä½¿ç”¨ç°æœ‰çš„è§£æé€»è¾‘
            articles = self.pmc_downloader.parse_full_articles(xml_text, f"batch_{batch_num}")

            # ä¿å­˜è§£æç»“æœ
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            json_file = Path(self.pmc_config.output_dir) / "parsed_json" / f"optimized_batch_{batch_num:05d}_{timestamp}.json"

            # å¢å¼ºæ–‡ç« æ•°æ®ï¼Œæ·»åŠ ç›¸å…³ç–¾ç—…ä¿¡æ¯
            for article in articles:
                article_dict = article.to_dict()
                pmc_id = article_dict.get('pmc_id', '')

                # æ·»åŠ ç›¸å…³ç–¾ç—…ä¿¡æ¯
                if pmc_id in self.literature_disease_mapping:
                    article_dict['related_diseases'] = self.literature_disease_mapping[pmc_id]

                    # ä¿å­˜åˆ°å…ƒæ•°æ®å­—å…¸
                    metadata = LiteratureMetadata(
                        pmc_id=article_dict.get('pmc_id', ''),
                        pmid=article_dict.get('pmid', ''),
                        title=article_dict.get('title', ''),
                        authors=article_dict.get('authors', []),
                        journal=article_dict.get('journal', ''),
                        publication_date=str(article_dict.get('publication_date', {})),
                        abstract=article_dict.get('abstract', ''),
                        doi=article_dict.get('doi', ''),
                        related_diseases=self.literature_disease_mapping[pmc_id]
                    )
                    self.literature_metadata[pmc_id] = metadata

                enhanced_articles.append(article_dict)

            # ä¿å­˜å¢å¼ºçš„JSONæ•°æ®
            batch_data = {
                'batch_number': batch_num,
                'search_timestamp': timestamp,
                'pmc_ids': pmc_ids,
                'total_articles': len(enhanced_articles),
                'articles': enhanced_articles,
                'optimization_info': {
                    'batch_type': 'deduplicated_batch',
                    'total_related_diseases': len(set().union(*[a.get('related_diseases', []) for a in enhanced_articles])),
                    'download_strategy': 'two_phase_optimization'
                }
            }

            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(batch_data, f, ensure_ascii=False, indent=2)

            print(f"   âœ… è§£æå¹¶ä¿å­˜: {json_file.name} ({len(enhanced_articles)} ç¯‡)")

        except Exception as e:
            print(f"   âŒ æ‰¹æ¬¡è§£æå¤±è´¥: {e}")

        return enhanced_articles

    def batch_download_pubmed_abstracts(self):
        """æ‰¹é‡ä¸‹è½½PubMedæ‘˜è¦"""
        if not self.unique_pmids:
            print("   â„¹ï¸  æ²¡æœ‰PubMedæ‘˜è¦éœ€è¦ä¸‹è½½")
            return

        # å°†PMIDè½¬æ¢ä¸ºåˆ—è¡¨å¹¶åˆ†æ‰¹å¤„ç†
        pmid_list = list(self.unique_pmids)
        batch_size = self.pubmed_config.batch_size

        total_batches = (len(pmid_list) + batch_size - 1) // batch_size
        successful_downloads = 0
        failed_downloads = 0
        all_articles = []

        print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   ğŸ“¦ æ€»æ‰¹æ¬¡æ•°: {total_batches}")

        for i in range(0, len(pmid_list), batch_size):
            batch_pmids = pmid_list[i:i + batch_size]
            batch_num = (i // batch_size) + 1

            print(f"   ğŸ“¥ ä¸‹è½½æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_pmids)} ç¯‡)...")

            try:
                # ä½¿ç”¨PubMedä¸‹è½½å™¨çš„æ‰¹é‡è·å–æ–¹æ³•
                articles = self.pubmed_downloader.fetch_abstracts_batch(batch_pmids, "optimized_batch")

                # å¢å¼ºæ–‡ç« æ•°æ®ï¼Œæ·»åŠ ç›¸å…³ç–¾ç—…ä¿¡æ¯
                enhanced_articles = []
                for article in articles:
                    article_dict = article.to_dict()
                    pmid = article_dict.get('pmid', '')

                    # æ·»åŠ ç›¸å…³ç–¾ç—…ä¿¡æ¯
                    if pmid in self.literature_disease_mapping:
                        article_dict['related_diseases'] = self.literature_disease_mapping[pmid]

                        # ä¿å­˜åˆ°å…ƒæ•°æ®å­—å…¸
                        metadata = LiteratureMetadata(
                            pmc_id=article_dict.get('pmcid', ''),
                            pmid=article_dict.get('pmid', ''),
                            title=article_dict.get('title', ''),
                            authors=article_dict.get('authors', []),
                            journal=article_dict.get('journal', ''),
                            publication_date=str(article_dict.get('publication_date', {})),
                            abstract=article_dict.get('abstract', ''),
                            doi=article_dict.get('doi', ''),
                            related_diseases=self.literature_disease_mapping[pmid]
                        )
                        self.literature_metadata[pmid] = metadata

                    enhanced_articles.append(article_dict)

                all_articles.extend(enhanced_articles)
                successful_downloads += len(articles)

                # ä¿å­˜æ‰¹æ¬¡æ•°æ®
                self.save_pubmed_batch_data(enhanced_articles, batch_pmids, batch_num)

                print(f"   âœ… æ‰¹æ¬¡ {batch_num} æˆåŠŸè·å– {len(articles)} ç¯‡æ‘˜è¦")

            except Exception as e:
                print(f"   âŒ æ‰¹æ¬¡ {batch_num} ä¸‹è½½å¤±è´¥: {e}")
                failed_downloads += len(batch_pmids)

            # è¯·æ±‚é—´éš”
            time.sleep(self.pubmed_downloader.get_sleep_time())

        # ä¿å­˜åˆå¹¶çš„æ•°æ®
        if all_articles:
            self.save_merged_pubmed_data(all_articles)

        print(f"   âœ… PubMedä¸‹è½½å®Œæˆ: æˆåŠŸ {successful_downloads}, å¤±è´¥ {failed_downloads}")

    def save_pubmed_batch_data(self, articles: List[Dict], pmids: List[str], batch_num: int):
        """ä¿å­˜PubMedæ‰¹æ¬¡æ•°æ®"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')

        # åˆ›å»ºæ‰¹æ¬¡ç›®å½•
        batch_dir = Path(self.pubmed_config.output_dir) / "batch_downloads"
        batch_dir.mkdir(exist_ok=True)

        # ä¿å­˜JSONæ•°æ®
        json_file = batch_dir / f"optimized_pubmed_batch_{batch_num:05d}_{timestamp}.json"

        batch_data = {
            'batch_number': batch_num,
            'search_timestamp': timestamp,
            'pmids': pmids,
            'total_articles': len(articles),
            'articles': articles,
            'optimization_info': {
                'batch_type': 'deduplicated_batch',
                'total_related_diseases': len(set().union(*[a.get('related_diseases', []) for a in articles])),
                'download_strategy': 'two_phase_optimization'
            }
        }

        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(batch_data, f, ensure_ascii=False, indent=2)

        print(f"   âœ… ä¿å­˜PubMedæ‰¹æ¬¡: {json_file.name} ({len(articles)} ç¯‡)")

    def save_merged_pubmed_data(self, all_articles: List[Dict]):
        """ä¿å­˜åˆå¹¶çš„PubMedæ•°æ®"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜åˆå¹¶çš„JSONæ•°æ®
        merged_file = Path(self.pubmed_config.output_dir) / f"optimized_pubmed_merged_{timestamp}.json"

        merged_data = {
            'merge_timestamp': timestamp,
            'total_articles': len(all_articles),
            'download_mode': self.download_mode,
            'unique_pmids_count': len(self.unique_pmids),
            'total_diseases': len(self.disease_literature_mapping),
            'articles': all_articles,
            'optimization_summary': {
                'strategy': 'two_phase_deduplication',
                'deduplicated_pmids': len(self.unique_pmids),
                'total_related_diseases': len(set().union(*[a.get('related_diseases', []) for a in all_articles]))
            }
        }

        with open(merged_file, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)

        print(f"   âœ… ä¿å­˜åˆå¹¶æ•°æ®: {merged_file.name} ({len(all_articles)} ç¯‡æ€»è®¡)")

        # ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆæŒ‰ç–¾ç—…åˆ†æ‹†ï¼‰
        self.save_articles_by_disease_csv(all_articles, timestamp)

        # ä¿å­˜ä¸ºç»Ÿä¸€çš„CSVæ–‡ä»¶
        self.save_unified_csv(all_articles, timestamp)

    def save_articles_by_disease_csv(self, all_articles: List[Dict], timestamp: str):
        """æŒ‰ç–¾ç—…åˆ†æ‹†ä¿å­˜ä¸ºCSVæ–‡ä»¶"""
        # åˆ›å»ºCSVç›®å½•
        csv_dir = Path(self.pubmed_config.output_dir) / "csv_by_disease"
        csv_dir.mkdir(exist_ok=True)

        # æŒ‰ç–¾ç—…åˆ†ç»„æ–‡ç« 
        disease_articles = defaultdict(list)
        for article in all_articles:
            related_diseases = article.get('related_diseases', [])
            if related_diseases:
                for disease in related_diseases:
                    disease_articles[disease].append(article)

        # ä¸ºæ¯ä¸ªç–¾ç—…ä¿å­˜CSVæ–‡ä»¶
        for disease, articles in disease_articles.items():
            csv_file = csv_dir / f"{disease.replace(' ', '_').replace('/', '_')}_{timestamp}.csv"

            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                import csv
                writer = csv.writer(f)

                # å†™å…¥æ ‡é¢˜è¡Œ
                writer.writerow([
                    'PMID', 'PMCID', 'Title', 'Abstract', 'Authors', 'Journal',
                    'Publication Year', 'DOI', 'MeSH Terms', 'Publication Types',
                    'Keywords', 'Related Diseases', 'Publication Date', 'Article Language'
                ])

                # å†™å…¥æ•°æ®
                for article in articles:
                    writer.writerow([
                        article.get('pmid', ''),
                        article.get('pmcid', ''),
                        article.get('title', '')[:1000] + '...' if len(article.get('title', '')) > 1000 else article.get('title', ''),
                        article.get('abstract', '')[:2000] + '...' if len(article.get('abstract', '')) > 2000 else article.get('abstract', ''),
                        '; '.join(article.get('authors', []))[:500] + '...' if len('; '.join(article.get('authors', []))) > 500 else '; '.join(article.get('authors', [])),
                        article.get('journal', ''),
                        article.get('publication_date', {}).get('year', ''),
                        article.get('doi', ''),
                        '; '.join(article.get('mesh_terms', [])),
                        '; '.join(article.get('publication_types', [])),
                        '; '.join(article.get('keywords', [])),
                        '; '.join(article.get('related_diseases', [])),
                        article.get('publication_date', {}).get('formatted', ''),
                        '; '.join(article.get('abstract_languages', []))
                    ])

            print(f"   ğŸ“Š ä¿å­˜ç–¾ç—…CSV: {csv_file.name} ({len(articles)} ç¯‡)")

    def save_unified_csv(self, all_articles: List[Dict], timestamp: str):
        """ä¿å­˜ç»Ÿä¸€çš„CSVæ–‡ä»¶"""
        unified_csv_file = Path(self.pubmed_config.output_dir) / f"optimized_pubmed_unified_{timestamp}.csv"

        with open(unified_csv_file, 'w', newline='', encoding='utf-8') as f:
            import csv
            writer = csv.writer(f)

            # å†™å…¥æ ‡é¢˜è¡Œ
            writer.writerow([
                'PMID', 'PMCID', 'Title', 'Abstract', 'Authors', 'Journal',
                'Publication Year', 'Publication Month', 'Publication Day',
                'DOI', 'MeSH Terms', 'Publication Types', 'Keywords',
                'Related Diseases', 'Abstract Languages', 'Disease Batch',
                'Download Timestamp'
            ])

            # å†™å…¥æ•°æ®
            for article in all_articles:
                writer.writerow([
                    article.get('pmid', ''),
                    article.get('pmcid', ''),
                    article.get('title', ''),
                    article.get('abstract', ''),
                    '; '.join(article.get('authors', [])),
                    article.get('journal', ''),
                    article.get('publication_date', {}).get('year', ''),
                    article.get('publication_date', {}).get('month', ''),
                    article.get('publication_date', {}).get('day', ''),
                    article.get('doi', ''),
                    '; '.join(article.get('mesh_terms', [])),
                    '; '.join(article.get('publication_types', [])),
                    '; '.join(article.get('keywords', [])),
                    '; '.join(article.get('related_diseases', [])),
                    '; '.join(article.get('abstract_languages', [])),
                    article.get('disease', 'optimized_batch'),
                    timestamp
                ])

        print(f"   ğŸ“‹ ä¿å­˜ç»Ÿä¸€CSV: {unified_csv_file.name} ({len(all_articles)} ç¯‡æ€»è®¡)")

    def save_pmc_csv_data(self, all_articles: List[Dict]):
        """ä¿å­˜PMCæ•°æ®ä¸ºCSVæ ¼å¼"""
        timestamp = time.strftime('%Y%m%d_%H%M%S')

        # ä¿å­˜ç»Ÿä¸€çš„PMC CSVæ–‡ä»¶
        unified_csv_file = Path(self.pmc_config.output_dir) / f"optimized_pmc_unified_{timestamp}.csv"

        with open(unified_csv_file, 'w', newline='', encoding='utf-8-sig') as f:
            import csv
            writer = csv.writer(f)

            # å†™å…¥æ ‡é¢˜è¡Œ
            writer.writerow([
                'PMC ID', 'PMID', 'Title', 'Abstract', 'Authors', 'Journal',
                'Publication Date', 'DOI', 'Full Text URL', 'Related Diseases',
                'Download Timestamp', 'Article Type', 'Language'
            ])

            # å†™å…¥æ•°æ®
            for article in all_articles:
                # å¤„ç†ä½œè€…åˆ—è¡¨
                authors = article.get('authors', [])
                authors_str = '; '.join(authors) if isinstance(authors, list) else str(authors)

                # å¤„ç†ç›¸å…³ç–¾ç—…åˆ—è¡¨
                related_diseases = article.get('related_diseases', [])
                diseases_str = '; '.join(related_diseases) if isinstance(related_diseases, list) else str(related_diseases)

                # æˆªæ–­è¿‡é•¿çš„å­—æ®µ
                title = article.get('title', '')
                if len(title) > 1000:
                    title = title[:1000] + '...'

                abstract = article.get('abstract', '')
                if len(abstract) > 2000:
                    abstract = abstract[:2000] + '...'

                # å¤„ç†å‘è¡¨æ—¥æœŸ
                pub_date = article.get('publication_date', {})
                if isinstance(pub_date, dict):
                    pub_date_str = f"{pub_date.get('year', '')}-{pub_date.get('month', '')}-{pub_date.get('day', '')}"
                else:
                    pub_date_str = str(pub_date)

                writer.writerow([
                    article.get('pmc_id', ''),
                    article.get('pmid', ''),
                    title,
                    abstract,
                    authors_str[:500] + '...' if len(authors_str) > 500 else authors_str,
                    article.get('journal', ''),
                    pub_date_str,
                    article.get('doi', ''),
                    article.get('full_text_url', ''),
                    diseases_str,
                    timestamp,
                    article.get('article_type', ''),
                    article.get('language', '')
                ])

        print(f"   ğŸ“Š ä¿å­˜PMCç»Ÿä¸€CSV: {unified_csv_file.name} ({len(all_articles)} ç¯‡æ€»è®¡)")

        # æŒ‰ç–¾ç—…åˆ†æ‹†ä¿å­˜CSVæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        self.save_pmc_csv_by_disease(all_articles, timestamp)

    def save_pmc_csv_by_disease(self, all_articles: List[Dict], timestamp: str):
        """æŒ‰ç–¾ç—…åˆ†æ‹†ä¿å­˜PMC CSVæ–‡ä»¶"""
        # åˆ›å»ºCSVç›®å½•
        csv_dir = Path(self.pmc_config.output_dir) / "csv_by_disease"
        csv_dir.mkdir(exist_ok=True)

        # æŒ‰ç–¾ç—…åˆ†ç»„æ–‡ç« 
        disease_articles = defaultdict(list)
        for article in all_articles:
            related_diseases = article.get('related_diseases', [])
            if related_diseases:
                for disease in related_diseases:
                    disease_articles[disease].append(article)

        # ä¸ºæ¯ä¸ªç–¾ç—…ä¿å­˜CSVæ–‡ä»¶
        for disease, articles in disease_articles.items():
            # æ¸…ç†ç–¾ç—…åç§°ç”¨äºæ–‡ä»¶å
            safe_disease_name = disease.replace(' ', '_').replace('/', '_').replace('\\', '_')
            csv_file = csv_dir / f"PMC_{safe_disease_name}_{timestamp}.csv"

            with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
                import csv
                writer = csv.writer(f)

                # å†™å…¥æ ‡é¢˜è¡Œ
                writer.writerow([
                    'PMC ID', 'PMID', 'Title', 'Abstract', 'Authors', 'Journal',
                    'Publication Date', 'DOI', 'Full Text URL', 'Download Timestamp'
                ])

                # å†™å…¥æ•°æ®
                for article in articles:
                    # å¤„ç†ä½œè€…åˆ—è¡¨
                    authors = article.get('authors', [])
                    authors_str = '; '.join(authors) if isinstance(authors, list) else str(authors)

                    # æˆªæ–­è¿‡é•¿çš„å­—æ®µ
                    title = article.get('title', '')
                    if len(title) > 1000:
                        title = title[:1000] + '...'

                    abstract = article.get('abstract', '')
                    if len(abstract) > 2000:
                        abstract = abstract[:2000] + '...'

                    # å¤„ç†å‘è¡¨æ—¥æœŸ
                    pub_date = article.get('publication_date', {})
                    if isinstance(pub_date, dict):
                        pub_date_str = f"{pub_date.get('year', '')}-{pub_date.get('month', '')}-{pub_date.get('day', '')}"
                    else:
                        pub_date_str = str(pub_date)

                    writer.writerow([
                        article.get('pmc_id', ''),
                        article.get('pmid', ''),
                        title,
                        abstract,
                        authors_str[:500] + '...' if len(authors_str) > 500 else authors_str,
                        article.get('journal', ''),
                        pub_date_str,
                        article.get('doi', ''),
                        article.get('full_text_url', ''),
                        timestamp
                    ])

            print(f"   ğŸ“Š ä¿å­˜PMCç–¾ç—…CSV: {csv_file.name} ({len(articles)} ç¯‡)")

    def save_optimization_report(self):
        """ä¿å­˜ä¼˜åŒ–æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.base_output_dir / f"optimization_report_{timestamp}.json"

        # ç»Ÿè®¡ä¿¡æ¯
        total_diseases = len(self.disease_literature_mapping)
        successful_diseases = sum(1 for info in self.disease_literature_mapping.values() if info.success)

        # è®¡ç®—é‡å¤æƒ…å†µ
        total_original_pmc = sum(len(info.pmc_ids) for info in self.disease_literature_mapping.values())
        total_original_pmid = sum(len(info.pmids) for info in self.disease_literature_mapping.values())

        report = {
            'optimization_summary': {
                'strategy': 'two_phase_deduplication',
                'timestamp': timestamp,
                'total_diseases_processed': total_diseases,
                'successful_diseases': successful_diseases,
                'success_rate': (successful_diseases / total_diseases * 100) if total_diseases > 0 else 0
            },
            'deduplication_stats': {
                'pmc_original_count': total_original_pmc,
                'pmc_deduplicated_count': len(self.unique_pmc_ids),
                'pmc_reduction_count': total_original_pmc - len(self.unique_pmc_ids),
                'pmc_reduction_percentage': ((total_original_pmc - len(self.unique_pmc_ids)) / total_original_pmc * 100) if total_original_pmc > 0 else 0,
                'pubmed_original_count': total_original_pmid,
                'pubmed_deduplicated_count': len(self.unique_pmids),
                'pubmed_reduction_count': total_original_pmid - len(self.unique_pmids),
                'pubmed_reduction_percentage': ((total_original_pmid - len(self.unique_pmids)) / total_original_pmid * 100) if total_original_pmid > 0 else 0
            },
            'literature_mapping': {
                'disease_count': len(self.disease_literature_mapping),
                'unique_pmc_count': len(self.unique_pmc_ids),
                'unique_pubmed_count': len(self.unique_pmids),
                'literature_disease_mappings': len(self.literature_disease_mapping)
            },
            'disease_details': {
                disease: {
                    'disease': info.disease,
                    'search_terms': info.search_terms,
                    'pmc_ids': info.pmc_ids,
                    'pmids': info.pmids,
                    'pmc_count': info.pmc_count,
                    'pmid_count': info.pmid_count,
                    'processing_time': info.processing_time,
                    'success': info.success,
                    'error': info.error
                } for disease, info in self.disease_literature_mapping.items()
            },
            'literature_disease_mapping': dict(self.literature_disease_mapping),
            'sample_literature_metadata': {
                pmc_id: {
                    'pmc_id': metadata.pmc_id,
                    'pmid': metadata.pmid,
                    'title': metadata.title,
                    'authors': metadata.authors,
                    'journal': metadata.journal,
                    'publication_date': metadata.publication_date,
                    'abstract': metadata.abstract,
                    'doi': metadata.doi,
                    'related_diseases': metadata.related_diseases
                } for pmc_id, metadata in list(self.literature_metadata.items())[:10]
            }
        }

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“Š ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        self.display_optimization_summary(report)

    def display_optimization_summary(self, report: Dict):
        """æ˜¾ç¤ºä¼˜åŒ–æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š æ–‡çŒ®ä¸‹è½½ä¼˜åŒ–æŠ¥å‘Š")
        print("="*80)

        summary = report['optimization_summary']
        dedup = report['deduplication_stats']
        mapping = report['literature_mapping']

        print(f"ğŸ“… ä¼˜åŒ–æ—¶é—´: {summary['timestamp']}")
        print(f"ğŸ”¬ å¤„ç†ç–¾ç—…: {summary['total_diseases_processed']}")
        print(f"âœ… æˆåŠŸæ”¶é›†: {summary['successful_diseases']} ({summary['success_rate']:.1f}%)")
        print()

        print("ğŸ¯ å»é‡æ•ˆæœ:")
        print(f"   ğŸ“š PMC: {dedup['pmc_original_count']} â†’ {dedup['pmc_deduplicated_count']} (å‡å°‘ {dedup['pmc_reduction_percentage']:.1f}%)")
        print(f"   ğŸ“„ PubMed: {dedup['pubmed_original_count']} â†’ {dedup['pubmed_deduplicated_count']} (å‡å°‘ {dedup['pubmed_reduction_percentage']:.1f}%)")
        print()

        print("ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   ğŸ”— ç–¾ç—…-æ–‡çŒ®æ˜ å°„: {mapping['literature_disease_mappings']} ä¸ª")
        print(f"   ğŸ“š å»é‡PMCæ–‡ç« : {mapping['unique_pmc_count']} ç¯‡")
        print(f"   ğŸ“„ å»é‡PubMedæ‘˜è¦: {mapping['unique_pubmed_count']} ç¯‡")
        print()

        print("ğŸ’¡ ä¼˜åŒ–ä¼˜åŠ¿:")
        print("   âœ… é¿å…é‡å¤ä¸‹è½½ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´")
        print("   âœ… å‡å°‘ç½‘ç»œè¯·æ±‚ï¼Œæé«˜ä¸‹è½½æ•ˆç‡")
        print("   âœ… å»ºç«‹æ¸…æ™°çš„ç–¾ç—…-æ–‡çŒ®æ˜ å°„å…³ç³»")
        print("   âœ… ä¾¿äºåç»­çš„æ•°æ®åˆ†æå’Œå¤„ç†")
        print("="*80)

    def run_optimized_download(self, diseases: List[str], max_diseases: Optional[int] = None):
        """è¿è¡Œä¼˜åŒ–ä¸‹è½½æµç¨‹"""
        print("ğŸ§¬ ä¼˜åŒ–ç‰ˆç½•è§ç–¾ç—…æ–‡çŒ®ä¸‹è½½å·¥å…·")
        print("ğŸ¯ é‡‡ç”¨ä¸¤é˜¶æ®µå»é‡ç­–ç•¥")
        print("="*50)

        try:
            # é˜¶æ®µä¸€ï¼šæ”¶é›†æ–‡çŒ®ID
            self.stage_one_collect_literature_ids(diseases, max_diseases)

            # é˜¶æ®µäºŒï¼šæ‰¹é‡ä¸‹è½½
            self.stage_two_batch_download()

            # ä¿å­˜ä¼˜åŒ–æŠ¥å‘Š
            self.save_optimization_report()

            print(f"\nğŸ‰ ä¼˜åŒ–ä¸‹è½½å®Œæˆï¼")
            print(f"ğŸ’¡ å¯æŸ¥çœ‹ä¸‹è½½çš„æ–‡çŒ®æ•°æ®å’Œä¼˜åŒ–æŠ¥å‘Š")

        except KeyboardInterrupt:
            print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº†ä¸‹è½½è¿‡ç¨‹")
            # å³ä½¿ä¸­æ–­ä¹Ÿè¦ä¿å­˜å·²æ”¶é›†çš„æ•°æ®
            if self.disease_literature_mapping:
                self.save_optimization_report()
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¬ ä¼˜åŒ–ç‰ˆç½•è§ç–¾ç—…æ–‡çŒ®ä¸‹è½½å·¥å…·")
    print("="*50)

    # é€‰æ‹©ä¸‹è½½æ¨¡å¼
    print("è¯·é€‰æ‹©ä¸‹è½½æ¨¡å¼:")
    print("1. ğŸ“„ ä»…ä¸‹è½½PubMedæ‘˜è¦")
    print("2. ğŸ“š ä»…ä¸‹è½½PMCå…¨æ–‡")
    print("3. ğŸ”„ åŒæ—¶ä¸‹è½½PubMedæ‘˜è¦å’ŒPMCå…¨æ–‡")

    while True:
        try:
            choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-3): ").strip()
            if choice == '1':
                download_mode = "pubmed_only"
                break
            elif choice == '2':
                download_mode = "pmc_only"
                break
            elif choice == '3':
                download_mode = "both"
                break
            else:
                print("âŒ è¯·è¾“å…¥ 1-3 ä¹‹é—´çš„æ•°å­—")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆï¼Œé€€å‡ºç¨‹åº")
            return

    print(f"\nğŸš€ é€‰æ‹©äº†ä¼˜åŒ–ä¸‹è½½æ¨¡å¼: {download_mode}")
    downloader = OptimizedLiteratureDownloader(download_mode)

    # åŠ è½½ç–¾ç—…åˆ—è¡¨
    diseases = downloader.load_disease_list()

    # è¯¢é—®ç”¨æˆ·è¦å¤„ç†å¤šå°‘ä¸ªç–¾ç—…
    print(f"\nğŸ’¡ ä¼˜åŒ–ç‰ˆä¸‹è½½æç¤º:")
    print(f"   - æµ‹è¯•å»ºè®®: 50-100 ä¸ªç–¾ç—…")
    print(f"   - ä¸­ç­‰è§„æ¨¡: 500-1000 ä¸ªç–¾ç—…")
    print(f"   - å…¨é‡ä¸‹è½½: {len(diseases)} ä¸ªç–¾ç—…")
    print(f"   - æŒ‰ Ctrl+C å¯éšæ—¶åœæ­¢")
    print(f"   - ä¼˜åŒ–ç‰ˆä¼šè‡ªåŠ¨å»é‡ï¼Œæé«˜æ•ˆç‡")

    while True:
        try:
            user_input = input(f"\nè¯·è¾“å…¥è¦å¤„ç†çš„ç–¾ç—…æ•°é‡ (1-{len(diseases)}, é»˜è®¤50): ").strip()
            if not user_input:
                max_diseases = 50
            else:
                max_diseases = int(user_input)
                if max_diseases < 1 or max_diseases > len(diseases):
                    print(f"âŒ è¯·è¾“å…¥ 1-{len(diseases)} ä¹‹é—´çš„æ•°å­—")
                    continue
            break
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·å–æ¶ˆï¼Œé€€å‡ºç¨‹åº")
            return

    print(f"\nğŸš€ å¼€å§‹ä¼˜åŒ–å¤„ç† {max_diseases} ä¸ªç–¾ç—…...")

    try:
        # è¿è¡Œä¼˜åŒ–ä¸‹è½½
        downloader.run_optimized_download(diseases, max_diseases)

    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­äº†ä¸‹è½½è¿‡ç¨‹")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()